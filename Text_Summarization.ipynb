{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e9bc00d-77ae-4784-88e0-868346745a60",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a31b36a-9d3d-4fd6-973e-6f26dc8e383c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92f2a804-f9a0-496f-8932-5d8a44cd1da3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4962781f-94dd-4f33-820e-3deddb021a10",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from difflib import SequenceMatcher\n",
    "from typing import List, Tuple\n",
    "import os\n",
    "\n",
    "def read_document_in_chunks(file_path: str, chunk_size: int = 4096) -> List[str]:\n",
    "    \"\"\"\n",
    "    Reads a document in chunks and returns a list of chunks.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the document file.\n",
    "        chunk_size (int, optional): The size of each chunk. Defaults to 4096.\n",
    "\n",
    "    Returns:\n",
    "        List[str]: A list of strings, where each string is a chunk of the document.\n",
    "                     Returns an empty list if the file cannot be read.\n",
    "    \"\"\"\n",
    "    chunks = []\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            while True:\n",
    "                chunk = file.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break\n",
    "                chunks.append(chunk)\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return []\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return []\n",
    "    return chunks\n",
    "\n",
    "def calculate_similarity(text1: str, text2: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculates the similarity between two texts using the SequenceMatcher.\n",
    "\n",
    "    Args:\n",
    "        text1 (str): The first text.\n",
    "        text2 (str): The second text.\n",
    "\n",
    "    Returns:\n",
    "        float: The similarity score, a value between 0 and 1 (inclusive).\n",
    "               Returns 0.0 if either input is empty.\n",
    "    \"\"\"\n",
    "    if not text1 or not text2:\n",
    "        return 0.0\n",
    "    return SequenceMatcher(None, text1, text2).ratio()\n",
    "\n",
    "def compare_documents_chunk_by_chunk(file_path1: str, file_path2: str) -> List[Tuple[Tuple[int, int], float]]:\n",
    "    \"\"\"\n",
    "    Compares two documents chunk by chunk and returns a list of similarity scores\n",
    "    for all possible chunk combinations.\n",
    "\n",
    "    Args:\n",
    "        file_path1 (str): The path to the first document.\n",
    "        file_path2 (str): The path to the second document.\n",
    "\n",
    "    Returns:\n",
    "        List[Tuple[Tuple[int, int], float]]: A list of tuples. Each tuple contains:\n",
    "            - A tuple of chunk indices (i, j), where i is the index of the chunk\n",
    "              from file1 and j is the index of the chunk from file2.\n",
    "            - The similarity score (float) between the two chunks.\n",
    "            Returns an empty list if either file is empty or an error occurs.\n",
    "    \"\"\"\n",
    "    chunks1 = read_document_in_chunks(file_path1)\n",
    "    chunks2 = read_document_in_chunks(file_path2)\n",
    "\n",
    "    if not chunks1 or not chunks2:\n",
    "        print(\"One or both documents are empty.\")\n",
    "        return []\n",
    "\n",
    "    results = []\n",
    "    for i, chunk1 in enumerate(chunks1):\n",
    "        for j, chunk2 in enumerate(chunks2):\n",
    "            similarity = calculate_similarity(chunk1, chunk2)\n",
    "            results.append(((i, j), similarity))\n",
    "    return results\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the usage of the\n",
    "    compare_documents_chunk_by_chunk function. Prompts the user for the file paths\n",
    "    and displays the comparison results.\n",
    "    \"\"\"\n",
    "    file_path1 = input(\"Enter the path to the first document: \")\n",
    "    file_path2 = input(\"Enter the path to the second document: \")\n",
    "\n",
    "    # Check if files exist before proceeding.\n",
    "    if not os.path.exists(file_path1):\n",
    "        print(f\"Error: File not found at {file_path1}\")\n",
    "        return\n",
    "    if not os.path.exists(file_path2):\n",
    "        print(f\"Error: File not found at {file_path2}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nComparing documents chunk by chunk...\")\n",
    "    results = compare_documents_chunk_by_chunk(file_path1, file_path2)\n",
    "\n",
    "    if not results:\n",
    "        print(\"No comparisons were made (either files are empty or there was an error).\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nChunk Comparison Results:\")\n",
    "    for (i, j), similarity in results:\n",
    "        print(f\"Chunk {i + 1} (Doc1) vs. Chunk {j + 1} (Doc2): Similarity = {similarity:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "78ed6403-b761-4a6f-a2cc-5602e15290c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the document file:  F:\\DSA_Task\\Natural_Language_Processing\\Don Wisidagama\\Language Technology Text Classification\\Product_Classification_Paper.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences containing numbers:\n",
      "An error occurred while reading the file: 'utf-8' codec can't decode byte 0xd2 in position 14: invalid continuation byte\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def extract_sentences_with_numbers(file_path, chunk_size=4096):\n",
    "    \"\"\"\n",
    "    Reads a long document in chunks, identifies sentences containing numbers,\n",
    "    and yields them. Handles potential issues with sentence boundary\n",
    "    detection within chunks.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the document file.\n",
    "        chunk_size (int, optional): The size of each chunk read from the file.\n",
    "            Defaults to 4096 (4KB). Adjust based on your system's memory\n",
    "            and the expected sentence length.\n",
    "\n",
    "    Yields:\n",
    "        str: Sentences from the document that contain at least one digit.\n",
    "             Yields one sentence at a time.\n",
    "    \"\"\"\n",
    "    # Regular expression to find any digit (0-9).\n",
    "    number_regex = re.compile(r'\\d')\n",
    "    # Regular expression to find sentence boundaries. Improved to handle\n",
    "    # more cases, including abbreviations, multiple punctuation, and\n",
    "    # non-ASCII characters.\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|!)\\s+|(?<=\\.|\\?|!)\"?\\s+')\n",
    "\n",
    "\n",
    "    try:\n",
    "        with open(file_path, 'r', encoding='utf-8') as file:\n",
    "            while True:\n",
    "                chunk = file.read(chunk_size)\n",
    "                if not chunk:\n",
    "                    break # End of file\n",
    "\n",
    "                # Split the chunk into sentences, handling potential issues\n",
    "                # where a sentence may span across chunk boundaries.\n",
    "                sentences = sentence_endings.split(chunk)\n",
    "                # Process sentences, one by one\n",
    "                for sentence in sentences:\n",
    "                    if number_regex.search(sentence):\n",
    "                        yield sentence.strip() # Remove leading/trailing spaces\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the file: {e}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the usage of the\n",
    "    extract_sentences_with_numbers function. Prompts the user for\n",
    "    the file path and processes the file.\n",
    "    \"\"\"\n",
    "    file_path = input(\"Enter the path to the document file: \")\n",
    "    # Check if file exists *before* passing to the generator.\n",
    "    import os\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File does not exist at the specified path: {file_path}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nSentences containing numbers:\")\n",
    "    for sentence in extract_sentences_with_numbers(file_path):\n",
    "        print(sentence) # Print each sentence as it's yielded\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ac9d77f4-f82c-4024-ba7f-fca6e47f302e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the Word document (.docx) file:  F:\\DSA_Task\\Natural_Language_Processing\\Don Wisidagama\\Language Technology Text Classification\\Product_Classification_Paper.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Sentences containing numbers:\n",
      "The results have shown that logistic regression model achieved Precision of 96.5% for one category and 90% for three categories.\n",
      "F1 score for IMDB dataset is 72.90%, 72.89% and 72.64% for 1, 3 and 5-grams model.\n",
      "For Amazon data, F1-score is 82.13%, 81.80% and 82.15% for 1-, 3- and 5-gram models.\n",
      "Here, n can be any positive integer and explained in detail as follows:\n",
      "Unigram or 1-gram is feature with one word in the document.\n",
      "Bigram or 2-grams is feature with two words in the document occurring together.\n",
      "Trigram or 3-grams is feature with three words in the document occurring together.\n",
      "The file has a header row followed by 45,895 rows containing an ID, a product title, text for the product description, and the product category.\n",
      "There is one row for missing title and 1042    rows for missing descriptions The rows with missing data are removed for further analysis.\n",
      "90% data included in the train data and remaining 10% in the test data.\n",
      "Results and Analysis: \n",
      "Hyper-parameter Selection - Hyper-parameter tuning is done by selection range for each parameter and selection of best hyper-parameter is achieved using 5-fold cross validation technique.\n",
      "Count-Vectorizer hyper-parameters - The settings for count vectorizer hyper-parameters are as follows:\n",
      "'vect__max_features': (500, 1000, 2500),\n",
      "'vect__ngram_range': [(1, 1), (1, 3)]  \n",
      "\n",
      "TF-IDF Hyper-parameters - The settings for count vectorizer hyper-parameters are as follows:\n",
      "'tfidf__max_features': (500, 1000, 2500)\n",
      "'tfidf__ngram_range': [(1, 1), (1, 3)]\n",
      "'tfidf__use_idf': [True, False]\n",
      "\n",
      "\n",
      "Logistic Regression Hyper-parameters - \n",
      "C - Regularization parameter to avoid the problem of overfitting.\n",
      "This parameter is set at high value of 1000 Iterations.\n",
      "Penalty function is set to L2 norm.\n",
      "Results and Analysis: \n",
      "5-fold grid search algorithm is used to obtain best set of hyper-parameters of bag of words model and logistic regression.\n",
      "Logistic Regression with TF-IDF model - The summary of the fitting process is given below for TF IDF model: best score of ROC-AUC is 96.5% where n-gram range is unigram, maximum features are 2500 and regularization parameter is 1.\n",
      "The classification report shows that logistic regression with TF-IDF model has precision and recall of 72% each.\n",
      "F1-score is 72%.\n",
      "Confusion Matrix:\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression with count vectorizer model - The summary of the fitting process is given below for count vectorizer model: best score of ROC-AUC is 95% where n-gram range is unigram, maximum features are 2500 and regularization parameter is 1000.\n",
      "The classification report shows that logistic regression with TF-IDF model has precision and recall of 67% and 68% respectively.\n",
      "F1-score is 67%.\n",
      "These metrics are precision, recall and F1-score.\n",
      "Each metric is calculated for each of the 21 target categories to assess model accuracy.\n",
      "TF-IDF model with logistic regression model has good performance in terms of F1-score, precision and recall.\n",
      "TF-IDF model has highest precision of 85% for cell phones & accessories and Pet Supplies.\n",
      "Appliances is next category with higher performance of 84% and 86% for precision and recall respectively.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document # Import the Document class from python-docx\n",
    "\n",
    "def extract_sentences_with_numbers(file_path):\n",
    "    \"\"\"\n",
    "    Reads a Word document, identifies sentences containing numbers,\n",
    "    and yields them.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .docx document file.\n",
    "\n",
    "    Yields:\n",
    "        str: Sentences from the document that contain at least one digit.\n",
    "             Yields one sentence at a time.\n",
    "    \"\"\"\n",
    "    # Regular expression to find any digit (0-9).\n",
    "    number_regex = re.compile(r'\\d')\n",
    "    # Regular expression to find sentence boundaries.\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|!)\\s+|(?<=\\.|\\?|!)\"?\\s+')\n",
    "\n",
    "    try:\n",
    "        # Load the Word document\n",
    "        document = Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in document.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        \n",
    "        # Join all paragraphs into a single string for sentence splitting\n",
    "        document_content = \"\\n\".join(full_text)\n",
    "\n",
    "        # Split the entire document content into sentences\n",
    "        sentences = sentence_endings.split(document_content)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if number_regex.search(sentence):\n",
    "                yield sentence.strip() # Remove leading/trailing spaces\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Word document: {e}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the usage of the\n",
    "    extract_sentences_with_numbers function. Prompts the user for\n",
    "    the file path and processes the file.\n",
    "    \"\"\"\n",
    "    file_path = input(\"Enter the path to the Word document (.docx) file: \")\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File does not exist at the specified path: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    if not file_path.lower().endswith('.docx'):\n",
    "        print(f\"Error: The provided file is not a .docx document. Please provide a Word document.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nSentences containing numbers:\")\n",
    "    for sentence in extract_sentences_with_numbers(file_path):\n",
    "        print(sentence)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d5443ec8-a6eb-40c3-8d92-591e30900e90",
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the Word document (.docx) file:  F:\\DSA_Task\\Natural_Language_Processing\\Don Wisidagama\\Language Technology Text Classification\\Product_Classification_Paper.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document for sentences with numbers...\n",
      "Sentence: The results have shown that logistic regression model achieved Precision of 96.5% for one category and 90% for three categories.\n",
      "  Extracted Numbers: [96.5, 90]\n",
      "\n",
      "Sentence: F1 score for IMDB dataset is 72.90%, 72.89% and 72.64% for 1, 3 and 5-grams model.\n",
      "  Extracted Numbers: [72.9, 72.89, 72.64, 1, 3, 5]\n",
      "\n",
      "Sentence: For Amazon data, F1-score is 82.13%, 81.80% and 82.15% for 1-, 3- and 5-gram models.\n",
      "  Extracted Numbers: [82.13, 81.8, 82.15, 1, 3, 5]\n",
      "\n",
      "Sentence: Here, n can be any positive integer and explained in detail as follows:\n",
      "Unigram or 1-gram is feature with one word in the document.\n",
      "  Extracted Numbers: [1]\n",
      "\n",
      "Sentence: Bigram or 2-grams is feature with two words in the document occurring together.\n",
      "  Extracted Numbers: [2]\n",
      "\n",
      "Sentence: Trigram or 3-grams is feature with three words in the document occurring together.\n",
      "  Extracted Numbers: [3]\n",
      "\n",
      "Sentence: The file has a header row followed by 45,895 rows containing an ID, a product title, text for the product description, and the product category.\n",
      "  Extracted Numbers: [45.895]\n",
      "\n",
      "Sentence: There is one row for missing title and 1042    rows for missing descriptions The rows with missing data are removed for further analysis.\n",
      "  Extracted Numbers: [1042]\n",
      "\n",
      "Sentence: 90% data included in the train data and remaining 10% in the test data.\n",
      "  Extracted Numbers: [90, 10]\n",
      "\n",
      "Sentence: Results and Analysis: \n",
      "Hyper-parameter Selection - Hyper-parameter tuning is done by selection range for each parameter and selection of best hyper-parameter is achieved using 5-fold cross validation technique.\n",
      "  Extracted Numbers: [5]\n",
      "\n",
      "Sentence: Count-Vectorizer hyper-parameters - The settings for count vectorizer hyper-parameters are as follows:\n",
      "'vect__max_features': (500, 1000, 2500),\n",
      "'vect__ngram_range': [(1, 1), (1, 3)]  \n",
      "\n",
      "TF-IDF Hyper-parameters - The settings for count vectorizer hyper-parameters are as follows:\n",
      "'tfidf__max_features': (500, 1000, 2500)\n",
      "'tfidf__ngram_range': [(1, 1), (1, 3)]\n",
      "'tfidf__use_idf': [True, False]\n",
      "\n",
      "\n",
      "Logistic Regression Hyper-parameters - \n",
      "C - Regularization parameter to avoid the problem of overfitting.\n",
      "  Extracted Numbers: [500, 1000, 2500, 1, 1, 1, 3, 500, 1000, 2500, 1, 1, 1, 3]\n",
      "\n",
      "Sentence: This parameter is set at high value of 1000 Iterations.\n",
      "  Extracted Numbers: [1000]\n",
      "\n",
      "Sentence: Penalty function is set to L2 norm.\n",
      "  Extracted Numbers: []\n",
      "\n",
      "Sentence: Results and Analysis: \n",
      "5-fold grid search algorithm is used to obtain best set of hyper-parameters of bag of words model and logistic regression.\n",
      "  Extracted Numbers: [5]\n",
      "\n",
      "Sentence: Logistic Regression with TF-IDF model - The summary of the fitting process is given below for TF IDF model: best score of ROC-AUC is 96.5% where n-gram range is unigram, maximum features are 2500 and regularization parameter is 1.\n",
      "  Extracted Numbers: [96.5, 2500, 1]\n",
      "\n",
      "Sentence: The classification report shows that logistic regression with TF-IDF model has precision and recall of 72% each.\n",
      "  Extracted Numbers: [72]\n",
      "\n",
      "Sentence: F1-score is 72%.\n",
      "  Extracted Numbers: [72]\n",
      "\n",
      "Sentence: Confusion Matrix:\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression with count vectorizer model - The summary of the fitting process is given below for count vectorizer model: best score of ROC-AUC is 95% where n-gram range is unigram, maximum features are 2500 and regularization parameter is 1000.\n",
      "  Extracted Numbers: [95, 2500, 1000]\n",
      "\n",
      "Sentence: The classification report shows that logistic regression with TF-IDF model has precision and recall of 67% and 68% respectively.\n",
      "  Extracted Numbers: [67, 68]\n",
      "\n",
      "Sentence: F1-score is 67%.\n",
      "  Extracted Numbers: [67]\n",
      "\n",
      "Sentence: These metrics are precision, recall and F1-score.\n",
      "  Extracted Numbers: []\n",
      "\n",
      "Sentence: Each metric is calculated for each of the 21 target categories to assess model accuracy.\n",
      "  Extracted Numbers: [21]\n",
      "\n",
      "Sentence: TF-IDF model with logistic regression model has good performance in terms of F1-score, precision and recall.\n",
      "  Extracted Numbers: []\n",
      "\n",
      "Sentence: TF-IDF model has highest precision of 85% for cell phones & accessories and Pet Supplies.\n",
      "  Extracted Numbers: [85]\n",
      "\n",
      "Sentence: Appliances is next category with higher performance of 84% and 86% for precision and recall respectively.\n",
      "  Extracted Numbers: [84, 86]\n",
      "\n",
      "\n",
      "--- Regarding Page Numbers ---\n",
      "Directly extracting page numbers from a .docx file using 'python-docx' is not feasible.\n",
      "This library works with the logical structure of the document, not its rendered layout.\n",
      "To get page numbers, you would typically need to:\n",
      "1. Convert the .docx to PDF, then use a PDF parsing library (e.g., PyPDF2, pdfminer.six) to extract text with positional information.\n",
      "2. Use a Word automation tool (e.g., 'pywin32' on Windows) to interact with Microsoft Word itself, which can be complex and platform-specific.\n",
      "This code only extracts sentences with numbers and the numbers themselves.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "# You would need a PDF library like 'PyPDF2' or 'pdfminer.six' for PDF conversion\n",
    "# and then text extraction with coordinates, which is a separate complex task.\n",
    "# from PyPDF2 import PdfReader # Example for PDF, not directly used here for docx page numbers\n",
    "\n",
    "def extract_sentences_and_numbers(file_path):\n",
    "    \"\"\"\n",
    "    Reads a Word document, identifies sentences containing numbers,\n",
    "    and yields them along with the extracted numbers.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .docx document file.\n",
    "\n",
    "    Yields:\n",
    "        tuple: A tuple containing:\n",
    "            - str: The sentence from the document that contains at least one digit.\n",
    "            - list: A list of integers or floats found within that sentence.\n",
    "    \"\"\"\n",
    "    # Regular expression to find any digit (0-9).\n",
    "    number_regex = re.compile(r'\\d')\n",
    "    # Regular expression to find all numbers (integers and floats) in a sentence.\n",
    "    # This will capture sequences of digits, optionally with a decimal point.\n",
    "    all_numbers_regex = re.compile(r'\\b\\d+(?:[\\.,]\\d+)?\\b') # Handles comma or period as decimal\n",
    "    # Regular expression to find sentence boundaries.\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|!)\\s+|(?<=\\.|\\?|!)\"?\\s+')\n",
    "\n",
    "    try:\n",
    "        document = Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in document.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        \n",
    "        document_content = \"\\n\".join(full_text)\n",
    "\n",
    "        sentences = sentence_endings.split(document_content)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if number_regex.search(sentence):\n",
    "                extracted_numbers = []\n",
    "                for match in all_numbers_regex.finditer(sentence):\n",
    "                    # Convert to float for potential decimal numbers, then back to int if whole\n",
    "                    try:\n",
    "                        num = float(match.group().replace(',', '.')) # Replace comma with period for conversion\n",
    "                        if num == int(num):\n",
    "                            extracted_numbers.append(int(num))\n",
    "                        else:\n",
    "                            extracted_numbers.append(num)\n",
    "                    except ValueError:\n",
    "                        # Should not happen with the regex, but good for robustness\n",
    "                        pass\n",
    "                yield sentence.strip(), extracted_numbers\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Word document: {e}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the usage of the\n",
    "    extract_sentences_and_numbers function. Prompts the user for\n",
    "    the file path and processes the file.\n",
    "    \"\"\"\n",
    "    file_path = input(\"Enter the path to the Word document (.docx) file: \")\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File does not exist at the specified path: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    if not file_path.lower().endswith('.docx'):\n",
    "        print(f\"Error: The provided file is not a .docx document. Please provide a Word document.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nProcessing document for sentences with numbers...\")\n",
    "    found_any = False\n",
    "    for sentence, numbers in extract_sentences_and_numbers(file_path):\n",
    "        found_any = True\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        print(f\"  Extracted Numbers: {numbers}\\n\")\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No sentences containing numbers were found in the document.\")\n",
    "\n",
    "    print(\"\\n--- Regarding Page Numbers ---\")\n",
    "    print(\"Directly extracting page numbers from a .docx file using 'python-docx' is not feasible.\")\n",
    "    print(\"This library works with the logical structure of the document, not its rendered layout.\")\n",
    "    print(\"To get page numbers, you would typically need to:\")\n",
    "    print(\"1. Convert the .docx to PDF, then use a PDF parsing library (e.g., PyPDF2, pdfminer.six) to extract text with positional information.\")\n",
    "    print(\"2. Use a Word automation tool (e.g., 'pywin32' on Windows) to interact with Microsoft Word itself, which can be complex and platform-specific.\")\n",
    "    print(\"This code only extracts sentences with numbers and the numbers themselves.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f262454-e207-4903-bbcc-efa8f279c55f",
   "metadata": {},
   "outputs": [],
   "source": [
    "#pip install transformers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af1e1682-edb7-4c7f-9096-ed4b620fa5ba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at dslim/bert-base-NER were not used when initializing BertForTokenClassification: ['bert.pooler.dense.bias', 'bert.pooler.dense.weight']\n",
      "- This IS expected if you are initializing BertForTokenClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForTokenClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter the path to the Word document (.docx) file:  F:\\DSA_Task\\Natural_Language_Processing\\Don Wisidagama\\Language Technology Text Classification\\Product_Classification_Paper.docx\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing document for sentences with numbers and tagging...\n",
      "Sentence: The results have shown that logistic regression model achieved Precision of 96.5% for one category and 90% for three categories.\n",
      "  Number: 96.5, Tag: Precision Metric\n",
      "  Number: 90, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: F1 score for IMDB dataset is 72.90%, 72.89% and 72.64% for 1, 3 and 5-grams model.\n",
      "  Number: 72.9, Tag: Score\n",
      "  Number: 72.89, Tag: Untagged Number\n",
      "  Number: 72.64, Tag: Untagged Number\n",
      "  Number: 1, Tag: Untagged Number\n",
      "  Number: 3, Tag: Untagged Number\n",
      "  Number: 5, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: For Amazon data, F1-score is 82.13%, 81.80% and 82.15% for 1-, 3- and 5-gram models.\n",
      "  Number: 82.13, Tag: F1-Score Metric\n",
      "  Number: 81.8, Tag: F1-Score Metric\n",
      "  Number: 82.15, Tag: Score\n",
      "  Number: 1, Tag: Untagged Number\n",
      "  Number: 3, Tag: Untagged Number\n",
      "  Number: 5, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: Here, n can be any positive integer and explained in detail as follows:\n",
      "Unigram or 1-gram is feature with one word in the document.\n",
      "  Number: 1, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: Bigram or 2-grams is feature with two words in the document occurring together.\n",
      "  Number: 2, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: Trigram or 3-grams is feature with three words in the document occurring together.\n",
      "  Number: 3, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: The file has a header row followed by 45,895 rows containing an ID, a product title, text for the product description, and the product category.\n",
      "  Number: 45.895, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: There is one row for missing title and 1042    rows for missing descriptions The rows with missing data are removed for further analysis.\n",
      "  Number: 1042, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: 90% data included in the train data and remaining 10% in the test data.\n",
      "  Number: 90, Tag: Untagged Number\n",
      "  Number: 10, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: Results and Analysis: \n",
      "Hyper-parameter Selection - Hyper-parameter tuning is done by selection range for each parameter and selection of best hyper-parameter is achieved using 5-fold cross validation technique.\n",
      "  Number: 5, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: Count-Vectorizer hyper-parameters - The settings for count vectorizer hyper-parameters are as follows:\n",
      "'vect__max_features': (500, 1000, 2500),\n",
      "'vect__ngram_range': [(1, 1), (1, 3)]  \n",
      "\n",
      "TF-IDF Hyper-parameters - The settings for count vectorizer hyper-parameters are as follows:\n",
      "'tfidf__max_features': (500, 1000, 2500)\n",
      "'tfidf__ngram_range': [(1, 1), (1, 3)]\n",
      "'tfidf__use_idf': [True, False]\n",
      "\n",
      "\n",
      "Logistic Regression Hyper-parameters - \n",
      "C - Regularization parameter to avoid the problem of overfitting.\n",
      "  Number: 500, Tag: Untagged Number\n",
      "  Number: 1000, Tag: Untagged Number\n",
      "  Number: 2500, Tag: Untagged Number\n",
      "  Number: 1, Tag: Untagged Number\n",
      "  Number: 1, Tag: Untagged Number\n",
      "  Number: 1, Tag: Untagged Number\n",
      "  Number: 3, Tag: Untagged Number\n",
      "  Number: 500, Tag: Untagged Number\n",
      "  Number: 1000, Tag: Untagged Number\n",
      "  Number: 2500, Tag: Untagged Number\n",
      "  Number: 1, Tag: Untagged Number\n",
      "  Number: 1, Tag: Untagged Number\n",
      "  Number: 1, Tag: Untagged Number\n",
      "  Number: 3, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: This parameter is set at high value of 1000 Iterations.\n",
      "  Number: 1000, Tag: Generic Value\n",
      "--------------------------------------------------\n",
      "Sentence: Results and Analysis: \n",
      "5-fold grid search algorithm is used to obtain best set of hyper-parameters of bag of words model and logistic regression.\n",
      "  Number: 5, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: Logistic Regression with TF-IDF model - The summary of the fitting process is given below for TF IDF model: best score of ROC-AUC is 96.5% where n-gram range is unigram, maximum features are 2500 and regularization parameter is 1.\n",
      "  Number: 96.5, Tag: Score\n",
      "  Number: 2500, Tag: Untagged Number\n",
      "  Number: 1, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: The classification report shows that logistic regression with TF-IDF model has precision and recall of 72% each.\n",
      "  Number: 72, Tag: Recall Metric\n",
      "--------------------------------------------------\n",
      "Sentence: F1-score is 72%.\n",
      "  Number: 72, Tag: F1-Score Metric\n",
      "--------------------------------------------------\n",
      "Sentence: Confusion Matrix:\n",
      "\n",
      "\n",
      "\n",
      "Logistic Regression with count vectorizer model - The summary of the fitting process is given below for count vectorizer model: best score of ROC-AUC is 95% where n-gram range is unigram, maximum features are 2500 and regularization parameter is 1000.\n",
      "  Number: 95, Tag: Score\n",
      "  Number: 2500, Tag: Untagged Number\n",
      "  Number: 1000, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: The classification report shows that logistic regression with TF-IDF model has precision and recall of 67% and 68% respectively.\n",
      "  Number: 67, Tag: Recall Metric\n",
      "  Number: 68, Tag: Recall Metric\n",
      "--------------------------------------------------\n",
      "Sentence: F1-score is 67%.\n",
      "  Number: 67, Tag: F1-Score Metric\n",
      "--------------------------------------------------\n",
      "Sentence: Each metric is calculated for each of the 21 target categories to assess model accuracy.\n",
      "  Number: 21, Tag: Untagged Number\n",
      "--------------------------------------------------\n",
      "Sentence: TF-IDF model has highest precision of 85% for cell phones & accessories and Pet Supplies.\n",
      "  Number: 85, Tag: Precision Metric\n",
      "--------------------------------------------------\n",
      "Sentence: Appliances is next category with higher performance of 84% and 86% for precision and recall respectively.\n",
      "  Number: 84, Tag: Precision Metric\n",
      "  Number: 86, Tag: Recall Metric\n",
      "--------------------------------------------------\n",
      "\n",
      "--- Important Note on Tagging Metrics with Hugging Face Transformers ---\n",
      "The tagging in this code uses simple keyword matching and a general NER model (if loaded).\n",
      "For robustly tagging numbers with specific semantic categories (e.g., 'Recall Metric', 'Precision Metric'),\n",
      "you would need to:\n",
      "1. **Create a custom dataset:** Manually annotate sentences where numbers are linked to your desired categories.\n",
      "2. **Fine-tune a Hugging Face Transformer model:** Train a model (e.g., BERT for token classification) on this custom dataset.\n",
      "   This is a significant machine learning project requiring labeled data and training resources.\n",
      "The current keyword-based approach is a basic heuristic and will not be as accurate or comprehensive.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from transformers import pipeline\n",
    "\n",
    "# Load a pre-trained NER pipeline.\n",
    "# Note: This model is trained on general entities (PERSON, ORG, LOC, MISC),\n",
    "# not specific metric types like 'Recall' or 'Precision'.\n",
    "# We'll use this for demonstration, but for your specific need, you'd need a custom-trained model.\n",
    "try:\n",
    "    # Attempt to load a general NER model. You might need to install 'dslim/bert-base-NER'\n",
    "    # if it's not cached, or choose another suitable general NER model.\n",
    "    ner_pipeline = pipeline(\"ner\", model=\"dslim/bert-base-NER\", aggregation_strategy=\"simple\")\n",
    "    #ner_pipeline = pipeline(\"ner\", aggregation_strategy=\"simple\")\n",
    "except Exception as e:\n",
    "    print(f\"Could not load NER model. Please ensure 'dslim/bert-base-NER' is available or installed: {e}\")\n",
    "    print(\"Falling back to keyword-based number extraction without HuggingFace tagging.\")\n",
    "    ner_pipeline = None # Set to None if model loading fails\n",
    "\n",
    "def extract_sentences_and_tag_numbers(file_path):\n",
    "    \"\"\"\n",
    "    Reads a Word document, identifies sentences containing numbers,\n",
    "    and attempts to tag these numbers based on surrounding keywords,\n",
    "    using a (conceptual) NER approach for illustration.\n",
    "\n",
    "    Args:\n",
    "        file_path (str): The path to the .docx document file.\n",
    "\n",
    "    Yields:\n",
    "        tuple: A tuple containing:\n",
    "            - str: The sentence from the document that contains at least one digit.\n",
    "            - list: A list of tuples, where each tuple contains (number, inferred_tag).\n",
    "    \"\"\"\n",
    "    number_regex = re.compile(r'\\d')\n",
    "    all_numbers_regex = re.compile(r'\\b\\d+(?:[\\.,]\\d+)?\\b') # Handles comma or period as decimal\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|!)\\s+|(?<=\\.|\\?|!)\"?\\s+')\n",
    "\n",
    "    # Define keywords for tagging. This is a *simplistic heuristic* for demonstration.\n",
    "    # A real solution requires a fine-tuned NER model.\n",
    "    keyword_tags = {\n",
    "        \"recall\": \"Recall Metric\",\n",
    "        \"precision\": \"Precision Metric\",\n",
    "        \"accuracy\": \"Accuracy Metric\",\n",
    "        \"f1-score\": \"F1-Score Metric\",\n",
    "        \"temperature\": \"Temperature Reading\",\n",
    "        \"cost\": \"Cost/Price\",\n",
    "        \"price\": \"Cost/Price\",\n",
    "        \"revenue\": \"Financial Value\",\n",
    "        \"profit\": \"Financial Value\",\n",
    "        \"loss\": \"Financial Value\",\n",
    "        \"rate\": \"Rate/Percentage\",\n",
    "        \"percentage\": \"Rate/Percentage\",\n",
    "        \"count\": \"Count/Quantity\",\n",
    "        \"quantity\": \"Count/Quantity\",\n",
    "        \"total\": \"Total Value\",\n",
    "        \"score\": \"Score\",\n",
    "        \"value\": \"Generic Value\" # Catch-all, less specific\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        document = Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in document.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        \n",
    "        document_content = \"\\n\".join(full_text)\n",
    "\n",
    "        sentences = sentence_endings.split(document_content)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            if number_regex.search(sentence):\n",
    "                tagged_numbers = []\n",
    "                numbers_in_sentence = []\n",
    "\n",
    "                # Extract all numbers first\n",
    "                for match in all_numbers_regex.finditer(sentence):\n",
    "                    try:\n",
    "                        num_str = match.group().replace(',', '.')\n",
    "                        num = float(num_str)\n",
    "                        if num == int(num):\n",
    "                            numbers_in_sentence.append((int(num), match.span()))\n",
    "                        else:\n",
    "                            numbers_in_sentence.append((num, match.span()))\n",
    "                    except ValueError:\n",
    "                        pass # Skip if conversion fails\n",
    "\n",
    "                # Attempt to tag each number\n",
    "                for num_value, num_span in numbers_in_sentence:\n",
    "                    inferred_tag = \"Untagged Number\" # Default tag\n",
    "\n",
    "                    # Heuristic 1: Check nearby keywords (simple window)\n",
    "                    start_index, end_index = num_span\n",
    "                    # Check text before the number (e.g., 20 characters before)\n",
    "                    context_before = sentence[max(0, start_index - 30):start_index].lower()\n",
    "                    # Check text after the number (e.g., 20 characters after)\n",
    "                    context_after = sentence[end_index:min(len(sentence), end_index + 30)].lower()\n",
    "\n",
    "                    found_keyword = False\n",
    "                    for keyword, tag in keyword_tags.items():\n",
    "                        if keyword in context_before or keyword in context_after:\n",
    "                            inferred_tag = tag\n",
    "                            found_keyword = True\n",
    "                            break\n",
    "                    \n",
    "                    if found_keyword:\n",
    "                        tagged_numbers.append((num_value, inferred_tag))\n",
    "                        continue # Move to the next number\n",
    "\n",
    "                    # Heuristic 2 (Conceptual): Use general NER model if available\n",
    "                    # This is for illustration. A general NER model won't output\n",
    "                    # \"Recall Metric\". It might tag \"Recall\" as O and \"0.85\" as O or MISC.\n",
    "                    # A *custom-trained NER model* would be needed here.\n",
    "                    if ner_pipeline:\n",
    "                        ner_results = ner_pipeline(sentence)\n",
    "                        for entity in ner_results:\n",
    "                            # Check if the number's span overlaps with a recognized entity span\n",
    "                            entity_start = entity['start']\n",
    "                            entity_end = entity['end']\n",
    "\n",
    "                            # Check if the number is part of an entity or very close\n",
    "                            # This logic is highly simplified and needs robust tuning for a real model\n",
    "                            if (entity_start <= start_index < entity_end) or \\\n",
    "                               (entity_start <= end_index <= entity_end) or \\\n",
    "                               (start_index <= entity_start and end_index >= entity_end):\n",
    "                                \n",
    "                                # This is where a custom NER model would give specific tags\n",
    "                                # For a general model, 'entity_group' might be 'MISC' or 'ORG' etc.\n",
    "                                # We're still relying on heuristics here.\n",
    "                                if entity['entity_group'] in [\"MISC\", \"LOC\", \"ORG\", \"PER\"]:\n",
    "                                    # Fallback if a general entity type is found near the number\n",
    "                                    inferred_tag = f\"General NER Tag: {entity['entity_group']}\"\n",
    "                                    # If the entity is a specific keyword (like \"Recall\" itself),\n",
    "                                    # then we could combine it with the number.\n",
    "                                    # This is where a fine-tuned model truly shines.\n",
    "                                    # For example, if \"recall\" was tagged as B-METRIC and 0.85 as I-METRIC\n",
    "                                    # then we would combine them.\n",
    "                                    break # Assume the first relevant NER tag is sufficient\n",
    "\n",
    "                    tagged_numbers.append((num_value, inferred_tag))\n",
    "                \n",
    "                if tagged_numbers: # Only yield if we found and potentially tagged numbers\n",
    "                    yield sentence.strip(), tagged_numbers\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Word document: {e}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate the usage of the\n",
    "    extract_sentences_and_tag_numbers function.\n",
    "    \"\"\"\n",
    "    file_path = input(\"Enter the path to the Word document (.docx) file: \")\n",
    "    \n",
    "    import os\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File does not exist at the specified path: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    if not file_path.lower().endswith('.docx'):\n",
    "        print(f\"Error: The provided file is not a .docx document. Please provide a Word document.\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nProcessing document for sentences with numbers and tagging...\")\n",
    "    found_any = False\n",
    "    for sentence, tagged_numbers in extract_sentences_and_tag_numbers(file_path):\n",
    "        found_any = True\n",
    "        print(f\"Sentence: {sentence}\")\n",
    "        for num, tag in tagged_numbers:\n",
    "            print(f\"  Number: {num}, Tag: {tag}\")\n",
    "        print(\"-\" * 50) # Separator for readability\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No sentences containing numbers were found in the document.\")\n",
    "\n",
    "    print(\"\\n--- Important Note on Tagging Metrics with Hugging Face Transformers ---\")\n",
    "    print(\"The tagging in this code uses simple keyword matching and a general NER model (if loaded).\")\n",
    "    print(\"For robustly tagging numbers with specific semantic categories (e.g., 'Recall Metric', 'Precision Metric'),\")\n",
    "    print(\"you would need to:\")\n",
    "    print(\"1. **Create a custom dataset:** Manually annotate sentences where numbers are linked to your desired categories.\")\n",
    "    print(\"2. **Fine-tune a Hugging Face Transformer model:** Train a model (e.g., BERT for token classification) on this custom dataset.\")\n",
    "    print(\"   This is a significant machine learning project requiring labeled data and training resources.\")\n",
    "    print(\"The current keyword-based approach is a basic heuristic and will not be as accurate or comprehensive.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc94f739-8014-486b-af6d-211143a0117f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8de43fb5-ccf8-4341-9e39-bf53cad4a6e4",
   "metadata": {},
   "source": [
    "### Performance Keyword and Performance value extraction\n",
    "#### Use any LLM model to extract performance keyword and its value from a sentence. Do not use heuristic based approach as doe earlier. The input will be word document"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55d8152-8bd8-4085-b109-88a5c168acf9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ee7df9-fd3f-4387-ad01-95e5d846f0fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc91e282-ebda-455a-8c16-f8dd32bbe0e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48897806-733e-4309-814a-47ac95d8d4fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d81bba6f-39f3-48f6-8095-f5a32eaef39b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc23996-0724-4b33-973a-1017792ab590",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "618f6f7b-74bd-4bcd-8f12-f86af8c9eec3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5572ded6-393d-4c36-90a9-02a84c40f243",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json # Ensure json is imported at the top for clarity\n",
    "\n",
    "# --- 1. Load the LLM for Extraction ---\n",
    "# Using HuggingFaceH4/zephyr-7b-beta as it's truly open and instruction-tuned.\n",
    "# This model will be downloaded to your local cache. No API key needed.\n",
    "# NOTE: This is a 7B model. It requires significant RAM/VRAM (e.g., 8GB+ VRAM, 16GB+ RAM).\n",
    "# If you face CUDA out of memory, try removing `torch_dtype=torch.bfloat16`\n",
    "# If you don't have a GPU, remove `device_map=\"auto\"`\n",
    "try:\n",
    "    #model_name = \"HuggingFaceH4/zephyr-7b-beta\"\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\"\n",
    "    #model_name = \"bert-base-uncased\"\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=torch.bfloat16, # Use bfloat16 for efficiency if your GPU supports it, or torch.float16\n",
    "        device_map=\"auto\" # Automatically place model layers on available devices (GPU/CPU)\n",
    "    )\n",
    "\n",
    "    # Create a text generation pipeline for the LLM\n",
    "    llm_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=100, # Limit the LLM's output length\n",
    "        do_sample=False,    # For deterministic output\n",
    "        temperature=0.0,    # For deterministic output\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(f\"Successfully loaded LLM: {model_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM model '{model_name}': {e}\")\n",
    "    print(\"This error usually indicates insufficient RAM/VRAM or a corrupted download.\")\n",
    "    print(\"If you are running on CPU, remove `torch_dtype=torch.bfloat16` and `device_map='auto'`.\")\n",
    "    print(\"Consider using a smaller model if you have limited resources (e.g., a 1B or 3B parameter model).\")\n",
    "    llm_pipeline = None # Indicate that the LLM is not available\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def extract_info_with_llm(sentence, llm_pipeline):\n",
    "    \"\"\"\n",
    "    Uses an LLM to extract performance keywords and their values from a sentence.\n",
    "\n",
    "    Args:\n",
    "        sentence (str): The input sentence.\n",
    "        llm_pipeline: The Hugging Face pipeline for text generation (LLM).\n",
    "\n",
    "    Returns:\n",
    "        list: A list of dictionaries, each containing {'keyword': str, 'value': float/int}.\n",
    "              Returns an empty list if no information is extracted.\n",
    "    \"\"\"\n",
    "    if not llm_pipeline:\n",
    "        return []\n",
    "\n",
    "    # Zephyr uses a specific chat template for best performance\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at extracting performance metrics from text. Extract keywords and their numerical values.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Extract performance keywords and their corresponding numerical values from the following sentence.\n",
    "        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\n",
    "        Output the information as a JSON list, where each item is an object with 'keyword' and 'value' fields. The value should be a number (integer or float).\n",
    "        If no performance keywords and values are found, output an empty JSON list [].\n",
    "\n",
    "        Sentence: \"{sentence}\"\n",
    "        Output:\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # Apply the tokenizer's chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    try:\n",
    "        generated_text = llm_pipeline(formatted_prompt, stop_sequence=[\"\\n\\n\", \"```\"])\n",
    "        print(generated_text)\n",
    "        \n",
    "        # Extract the relevant part of the generated text\n",
    "        # LLMs often output the prompt again, then their answer.\n",
    "        # We need to find the part after the prompt and look for the JSON.\n",
    "        llm_output = generated_text[0]['generated_text'].replace(formatted_prompt, \"\").strip()\n",
    "        print(llm_output)\n",
    "\n",
    "        # Try to parse the JSON output\n",
    "        json_start = llm_output.find('[')\n",
    "        json_end = llm_output.rfind(']')\n",
    "\n",
    "        if json_start != -1 and json_end != -1 and json_end > json_start:\n",
    "            json_string = llm_output[json_start : json_end + 1]\n",
    "            json_string = json_string.strip()\n",
    "\n",
    "            try:\n",
    "                extracted_data = json.loads(json_string)\n",
    "                if isinstance(extracted_data, list):\n",
    "                    valid_extractions = []\n",
    "                    for item in extracted_data:\n",
    "                        if isinstance(item, dict) and 'keyword' in item and 'value' in item:\n",
    "                            try:\n",
    "                                item['value'] = float(item['value'])\n",
    "                                if item['value'] == int(item['value']):\n",
    "                                    item['value'] = int(item['value'])\n",
    "                                valid_extractions.append(item)\n",
    "                            except ValueError:\n",
    "                                pass # Skip if value is not a valid number\n",
    "                    return valid_extractions\n",
    "                return []\n",
    "            except json.JSONDecodeError as jde:\n",
    "                print(f\"Warning: Could not parse JSON from LLM output: {jde}\")\n",
    "                print(f\"LLM Raw Output snippet: {llm_output[:200]}...\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"Warning: LLM did not output expected JSON structure. Raw output snippet: {llm_output[:200]}...\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during LLM inference: {e}\")\n",
    "        return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "dd48bbf4-1c38-4034-9139-18a5674207e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Running Test Cases ---\n",
      "\n",
      "Test Case 1:\n",
      "Sentence: \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.0` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\generation\\configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.9` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'generated_text': '<|system|>\\nYou are an expert at extracting performance metrics from text. Extract keywords and their numerical values.</s>\\n<|user|>\\nExtract performance keywords and their corresponding numerical values from the following sentence.\\n        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\\n        Output the information as a JSON list, where each item is an object with \\'keyword\\' and \\'value\\' fields. The value should be a number (integer or float).\\n        If no performance keywords and values are found, output an empty JSON list [].\\n\\n        Sentence: \"The model achieved a recall of 0.92 and a precision of 0.88.\"\\n        Output:\\n        </s>\\n<|assistant|>\\nJSON response:\\n'}]\n",
      "JSON response:\n",
      "Warning: LLM did not output expected JSON structure. Raw output snippet: JSON response:...\n",
      "No performance metrics extracted.\n",
      "======================================================================\n",
      "\n",
      "Test Case 2:\n",
      "Sentence: \"Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\"\n",
      "[{'generated_text': '<|system|>\\nYou are an expert at extracting performance metrics from text. Extract keywords and their numerical values.</s>\\n<|user|>\\nExtract performance keywords and their corresponding numerical values from the following sentence.\\n        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\\n        Output the information as a JSON list, where each item is an object with \\'keyword\\' and \\'value\\' fields. The value should be a number (integer or float).\\n        If no performance keywords and values are found, output an empty JSON list [].\\n\\n        Sentence: \"Our system\\'s throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\"\\n        Output:\\n        </s>\\n<|assistant|>\\nResponse:\\n'}]\n",
      "Response:\n",
      "Warning: LLM did not output expected JSON structure. Raw output snippet: Response:...\n",
      "No performance metrics extracted.\n",
      "======================================================================\n",
      "\n",
      "Test Case 3:\n",
      "Sentence: \"The error rate was 0.01% in the last quarter.\"\n",
      "[{'generated_text': '<|system|>\\nYou are an expert at extracting performance metrics from text. Extract keywords and their numerical values.</s>\\n<|user|>\\nExtract performance keywords and their corresponding numerical values from the following sentence.\\n        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\\n        Output the information as a JSON list, where each item is an object with \\'keyword\\' and \\'value\\' fields. The value should be a number (integer or float).\\n        If no performance keywords and values are found, output an empty JSON list [].\\n\\n        Sentence: \"The error rate was 0.01% in the last quarter.\"\\n        Output:\\n        </s>\\n<|assistant|>\\nSample JSON output:\\n'}]\n",
      "Sample JSON output:\n",
      "Warning: LLM did not output expected JSON structure. Raw output snippet: Sample JSON output:...\n",
      "No performance metrics extracted.\n",
      "======================================================================\n",
      "\n",
      "Test Case 4:\n",
      "Sentence: \"Expected uptime is 99.9% for the server.\"\n",
      "[{'generated_text': '<|system|>\\nYou are an expert at extracting performance metrics from text. Extract keywords and their numerical values.</s>\\n<|user|>\\nExtract performance keywords and their corresponding numerical values from the following sentence.\\n        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\\n        Output the information as a JSON list, where each item is an object with \\'keyword\\' and \\'value\\' fields. The value should be a number (integer or float).\\n        If no performance keywords and values are found, output an empty JSON list [].\\n\\n        Sentence: \"Expected uptime is 99.9% for the server.\"\\n        Output:\\n        </s>\\n<|assistant|>\\nJSON response:\\n'}]\n",
      "JSON response:\n",
      "Warning: LLM did not output expected JSON structure. Raw output snippet: JSON response:...\n",
      "No performance metrics extracted.\n",
      "======================================================================\n",
      "\n",
      "Test Case 5:\n",
      "Sentence: \"The project cost is $1,250,000.\"\n",
      "[{'generated_text': '<|system|>\\nYou are an expert at extracting performance metrics from text. Extract keywords and their numerical values.</s>\\n<|user|>\\nExtract performance keywords and their corresponding numerical values from the following sentence.\\n        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\\n        Output the information as a JSON list, where each item is an object with \\'keyword\\' and \\'value\\' fields. The value should be a number (integer or float).\\n        If no performance keywords and values are found, output an empty JSON list [].\\n\\n        Sentence: \"The project cost is $1,250,000.\"\\n        Output:\\n        </s>\\n<|assistant|>\\n```\\n'}]\n",
      "```\n",
      "Warning: LLM did not output expected JSON structure. Raw output snippet: ```...\n",
      "No performance metrics extracted.\n",
      "======================================================================\n",
      "\n",
      "Test Case 6:\n",
      "Sentence: \"This sentence has no specific performance metrics.\"\n",
      "[{'generated_text': '<|system|>\\nYou are an expert at extracting performance metrics from text. Extract keywords and their numerical values.</s>\\n<|user|>\\nExtract performance keywords and their corresponding numerical values from the following sentence.\\n        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\\n        Output the information as a JSON list, where each item is an object with \\'keyword\\' and \\'value\\' fields. The value should be a number (integer or float).\\n        If no performance keywords and values are found, output an empty JSON list [].\\n\\n        Sentence: \"This sentence has no specific performance metrics.\"\\n        Output:\\n        </s>\\n<|assistant|>\\nResponse:\\n'}]\n",
      "Response:\n",
      "Warning: LLM did not output expected JSON structure. Raw output snippet: Response:...\n",
      "No performance metrics extracted.\n",
      "======================================================================\n",
      "\n",
      "Test Case 7:\n",
      "Sentence: \"Accuracy improved to 95.5%.\"\n",
      "[{'generated_text': '<|system|>\\nYou are an expert at extracting performance metrics from text. Extract keywords and their numerical values.</s>\\n<|user|>\\nExtract performance keywords and their corresponding numerical values from the following sentence.\\n        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\\n        Output the information as a JSON list, where each item is an object with \\'keyword\\' and \\'value\\' fields. The value should be a number (integer or float).\\n        If no performance keywords and values are found, output an empty JSON list [].\\n\\n        Sentence: \"Accuracy improved to 95.5%.\"\\n        Output:\\n        </s>\\n<|assistant|>\\nJSON response:\\n'}]\n",
      "JSON response:\n",
      "Warning: LLM did not output expected JSON structure. Raw output snippet: JSON response:...\n",
      "No performance metrics extracted.\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Test Cases ---\n",
    "if llm_pipeline: # Only run tests if the LLM loaded successfully\n",
    "    print(\"\\n--- Running Test Cases ---\")\n",
    "\n",
    "    test_sentences = [\n",
    "        \"The model achieved a recall of 0.92 and a precision of 0.88.\",\n",
    "        \"Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\",\n",
    "        \"The error rate was 0.01% in the last quarter.\",\n",
    "        \"Expected uptime is 99.9% for the server.\",\n",
    "        \"The project cost is $1,250,000.\", # LLM might not recognize \"cost\" as performance metric\n",
    "        \"This sentence has no specific performance metrics.\",\n",
    "        \"Accuracy improved to 95.5%.\"\n",
    "    ]\n",
    "\n",
    "    for i, sentence in enumerate(test_sentences):\n",
    "        print(f\"\\nTest Case {i+1}:\")\n",
    "        print(f\"Sentence: \\\"{sentence}\\\"\")\n",
    "        \n",
    "        extracted_metrics = extract_info_with_llm(sentence, llm_pipeline)\n",
    "        \n",
    "        if extracted_metrics:\n",
    "            print(\"Extracted Metrics:\")\n",
    "            for item in extracted_metrics:\n",
    "                print(f\"  - Keyword: '{item['keyword']}', Value: {item['value']}\")\n",
    "        else:\n",
    "            print(\"No performance metrics extracted.\")\n",
    "        print(\"=\" * 70)\n",
    "else:\n",
    "    print(\"\\nSkipping test cases because the LLM pipeline failed to load.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1dadb2d7-f0ae-4bbc-a883-128f1201aeb2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TinyLlama/TinyLlama-1.1B-Chat-v1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc0aad97-3822-438e-a456-a66bf1052cdd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75b3d7eb-6849-49ab-b130-c7860bd1ce17",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afc954f6-3d47-41cc-941a-0f867bf24c25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1f707-dacc-472f-8731-a31052c8753c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "bc3abd96-31e0-4285-bb46-8382e1c8fd60",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0\n",
      "\n",
      "Processing document 'dummy_doc.docx' for performance metrics using LLM...\n",
      "\n",
      "Processing Sentence: \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"The error rate was 0.01% in the last quarter.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Expected uptime is 99.9% for the server.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Accuracy improved to 95.5%.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Project budget was $1,250,000.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Client satisfaction improved by 20%.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "No performance metrics found in the entire document using the LLM.\n",
      "\n",
      "--- LLM-based Extraction Notes ---\n",
      "This approach leverages the LLM's understanding of instructions and context.\n",
      "Performance (speed and accuracy) will depend on:\n",
      "1. The chosen LLM model (smaller models are faster but less accurate).\n",
      "2. Your hardware (GPU is highly recommended for LLMs).\n",
      "3. The clarity and specificity of the prompt engineering.\n",
      "Errors in LLM output (e.g., malformed JSON) are possible and handled with error checks.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os\n",
    "\n",
    "# --- 1. Load the LLM for Extraction ---\n",
    "llm_pipeline = None\n",
    "\n",
    "try:\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "\n",
    "    # --- IMPORTANT ADJUSTMENTS FOR COMPATIBILITY ---\n",
    "    # 1. For CPU-only or older/incompatible GPUs:\n",
    "    #    Remove `torch_dtype` and `device_map=\"auto\"`.\n",
    "    #    This will load the model in float32 (default) on your CPU.\n",
    "    #    It will be slower than GPU, but highly compatible.\n",
    "    \n",
    "    # 2. If you have a modern NVIDIA GPU that supports bfloat16:\n",
    "    #    You can keep `torch_dtype=torch.bfloat16` and `device_map=\"auto\"`.\n",
    "    \n",
    "    # 3. If you have a modern NVIDIA GPU that supports float16 but NOT bfloat16:\n",
    "    #    Change `torch_dtype=torch.float16` and keep `device_map=\"auto\"`.\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        # COMMENT OUT OR REMOVE THESE LINES IF YOU ARE ON CPU OR GET ERRORS RELATED TO BFLOAT16/DEVICE\n",
    "        # torch_dtype=torch.bfloat16,\n",
    "        # device_map=\"auto\" \n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    llm_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=0.9,\n",
    "        # Ensure that the model is loaded with no special quantization config unless specified by user.\n",
    "        # If you were using 4-bit quantization, you'd add:\n",
    "        # quantization_config=bnb_config,\n",
    "    )\n",
    "    print(f\"Successfully loaded LLM: {model_name}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM model '{model_name}': {e}\")\n",
    "    print(\"\\n--- Troubleshooting Tips for LLM Loading ---\")\n",
    "    print(\"1. **Check RAM/VRAM:** Ensure you have enough memory (e.g., 2GB+ for TinyLlama).\")\n",
    "    print(\"2. **CPU Compatibility:** If on CPU, **remove `torch_dtype` and `device_map='auto'`** from `AutoModelForCausalLM.from_pretrained`.\")\n",
    "    print(\"3. **GPU `torch_dtype`:** If on GPU, try `torch_dtype=torch.float16` if `bfloat16` fails.\")\n",
    "    print(\"4. **Corrupted Download:** If it persists, delete the model from your Hugging Face cache and try again.\")\n",
    "    print(\"   (e.g., `huggingface-cli delete-cache TinyLlama/TinyLlama-1.1B-Chat-v1.0`)\")\n",
    "    llm_pipeline = None\n",
    "\n",
    "\n",
    "def extract_info_with_llm(sentence, llm_pipeline):\n",
    "    \"\"\"\n",
    "    Uses an LLM to extract performance keywords and their values from a sentence.\n",
    "    \"\"\"\n",
    "    if not llm_pipeline:\n",
    "        return []\n",
    "\n",
    "    # Simplified prompt for TinyLlama: Direct user message.\n",
    "    # Smaller models sometimes struggle with the \"system\" role.\n",
    "    messages = [\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Extract performance keywords and their corresponding numerical values from the following sentence.\n",
    "        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\n",
    "        Output the information as a JSON list, where each item is an object with 'keyword' and 'value' fields. The value should be a number (integer or float).\n",
    "        If no performance keywords and values are found, output an empty JSON list [].\n",
    "\n",
    "        Sentence: \"{sentence}\"\n",
    "        Output:\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # Apply the tokenizer's chat template\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    try:\n",
    "        # Crucial for small models: Loosen stop_sequence or remove it.\n",
    "        # Let's try to let it generate more freely and rely on our parsing.\n",
    "        # TinyLlama might also generate extra text *before* the JSON.\n",
    "        generated_text = llm_pipeline(formatted_prompt, stop_sequence=[\"\\n\\n\", \"```\", \"\\nOutput:\", \"\\n[]\"]) # Added more specific stop sequences for common LLM output patterns\n",
    "        \n",
    "        # --- DEBUGGING PRINTS ---\n",
    "        print(\"\\n--- LLM Raw Full Output (for debugging) ---\")\n",
    "        # Ensure we're printing the text attribute, not the entire dict\n",
    "        print(generated_text[0]['generated_text']) \n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "        # The replace method might be too strict if the LLM adds extra space/newlines.\n",
    "        # Instead, let's just look for the part after the prompt.\n",
    "        raw_llm_response = generated_text[0]['generated_text'].replace(formatted_prompt, \"\").strip()\n",
    "        \n",
    "        # --- DEBUGGING PRINTS ---\n",
    "        print(\"\\n--- LLM Response After Prompt Removal ---\")\n",
    "        print(raw_llm_response)\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "        # --- Enhanced JSON Parsing ---\n",
    "        # 1. Look for markdown code blocks (common for structured LLM output)\n",
    "        json_code_block_match = re.search(r'```json\\s*(.*?)\\s*```', raw_llm_response, re.DOTALL)\n",
    "        if json_code_block_match:\n",
    "            json_string_candidate = json_code_block_match.group(1)\n",
    "            print(\"Found JSON in code block.\") # Debugging\n",
    "        else:\n",
    "            json_string_candidate = raw_llm_response\n",
    "            print(\"No JSON code block found, trying direct parse.\") # Debugging\n",
    "\n",
    "        # 2. Look for the first '[' and last ']' in the *candidate* string\n",
    "        json_start = json_string_candidate.find('[')\n",
    "        json_end = json_string_candidate.rfind(']')\n",
    "\n",
    "        if json_start != -1 and json_end != -1 and json_end > json_start:\n",
    "            json_string = json_string_candidate[json_start : json_end + 1]\n",
    "            json_string = json_string.strip()\n",
    "            print(f\"Attempting to parse JSON string: {json_string}\") # Debugging\n",
    "\n",
    "            try:\n",
    "                extracted_data = json.loads(json_string)\n",
    "                if isinstance(extracted_data, list):\n",
    "                    valid_extractions = []\n",
    "                    for item in extracted_data:\n",
    "                        if isinstance(item, dict) and 'keyword' in item and 'value' in item:\n",
    "                            try:\n",
    "                                item['value'] = float(item['value'])\n",
    "                                if item['value'] == int(item['value']):\n",
    "                                    item['value'] = int(item['value'])\n",
    "                                valid_extractions.append(item)\n",
    "                            except ValueError:\n",
    "                                # Skip if value is not a valid number\n",
    "                                print(f\"Warning: Invalid value '{item.get('value')}' for keyword '{item.get('keyword')}'. Skipping.\") # Debugging\n",
    "                                pass \n",
    "                    return valid_extractions\n",
    "                print(\"Warning: JSON parsed but not a list.\") # Debugging\n",
    "                return []\n",
    "            except json.JSONDecodeError as jde:\n",
    "                print(f\"Warning: Could not parse JSON from LLM output: {jde}\")\n",
    "                print(f\"LLM Raw Output snippet causing error: {json_string_candidate[:200]}...\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"Warning: LLM did not output expected JSON structure ([...]). Raw output snippet: {raw_llm_response[:200]}...\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during LLM inference: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Rest of your existing document processing code (unchanged) ---\n",
    "\n",
    "def extract_sentences_from_docx(file_path):\n",
    "    \"\"\"\n",
    "    Reads a Word document and yields sentences.\n",
    "    \"\"\"\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|!)\\s+|(?<=\\.|\\?|!)\"?\\s+')\n",
    "\n",
    "    try:\n",
    "        document = Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in document.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        \n",
    "        document_content = \"\\n\".join(full_text)\n",
    "        sentences = sentence_endings.split(document_content)\n",
    "        \n",
    "        for sentence in sentences:\n",
    "            stripped_sentence = sentence.strip()\n",
    "            if stripped_sentence:\n",
    "                yield stripped_sentence\n",
    "\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Word document: {e}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    \"\"\"\n",
    "    Main function to demonstrate LLM-based extraction from a Word document.\n",
    "    \"\"\"\n",
    "    if not llm_pipeline:\n",
    "        print(\"LLM pipeline not initialized. Cannot perform LLM-based extraction.\")\n",
    "        return\n",
    "\n",
    "    # Use a dummy .docx file for testing if you don't have one readily available\n",
    "    # Create a dummy_doc.docx with some text for testing:\n",
    "    # \"The model achieved a recall of 0.92 and a precision of 0.88. Our system's throughput reached 1200 requests per second.\"\n",
    "    dummy_doc_path = \"dummy_doc.docx\"\n",
    "    import os\n",
    "    if not os.path.exists(dummy_doc_path):\n",
    "        print(f\"\\n--- Creating a dummy '{dummy_doc_path}' for testing ---\")\n",
    "        doc = Document()\n",
    "        doc.add_paragraph(\"The model achieved a recall of 0.92 and a precision of 0.88. Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\")\n",
    "        doc.add_paragraph(\"The error rate was 0.01% in the last quarter. Expected uptime is 99.9% for the server.\")\n",
    "        doc.add_paragraph(\"This sentence has no specific performance metrics. Accuracy improved to 95.5%.\")\n",
    "        doc.add_paragraph(\"Project budget was $1,250,000. Client satisfaction improved by 20%.\") # Testing for non-performance metrics\n",
    "        doc.save(dummy_doc_path)\n",
    "        print(f\"Dummy document created at: {dummy_doc_path}\")\n",
    "\n",
    "    # file_path = input(\"Enter the path to the Word document (.docx) file: \") # Use dummy for quick test\n",
    "    file_path = dummy_doc_path # Use the generated dummy document\n",
    "\n",
    "    import os\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File does not exist at the specified path: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    if not file_path.lower().endswith('.docx'):\n",
    "        print(f\"Error: The provided file is not a .docx document. Please provide a Word document.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing document '{os.path.basename(file_path)}' for performance metrics using LLM...\")\n",
    "    found_any = False\n",
    "    \n",
    "    number_regex = re.compile(r'\\d') # To filter sentences for LLM calls\n",
    "\n",
    "    for sentence in extract_sentences_from_docx(file_path):\n",
    "        if number_regex.search(sentence):\n",
    "            print(f\"\\nProcessing Sentence: \\\"{sentence}\\\"\") # Debugging: show which sentence is being processed\n",
    "            extracted_metrics = extract_info_with_llm(sentence, llm_pipeline)\n",
    "            \n",
    "            if extracted_metrics:\n",
    "                found_any = True\n",
    "                print(\"Extracted Metrics:\")\n",
    "                for item in extracted_metrics:\n",
    "                    print(f\"  - Keyword: '{item['keyword']}', Value: {item['value']}\")\n",
    "                print(\"=\" * 70)\n",
    "            else:\n",
    "                print(\"No performance metrics extracted for this sentence.\")\n",
    "                print(\"=\" * 70)\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No performance metrics found in the entire document using the LLM.\")\n",
    "\n",
    "    print(\"\\n--- LLM-based Extraction Notes ---\")\n",
    "    print(\"This approach leverages the LLM's understanding of instructions and context.\")\n",
    "    print(\"Performance (speed and accuracy) will depend on:\")\n",
    "    print(\"1. The chosen LLM model (smaller models are faster but less accurate).\")\n",
    "    print(\"2. Your hardware (GPU is highly recommended for LLMs).\")\n",
    "    print(\"3. The clarity and specificity of the prompt engineering.\")\n",
    "    print(\"Errors in LLM output (e.g., malformed JSON) are possible and handled with error checks.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9b376d56-cbbe-49bd-9037-36996ecb9ad2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0 on CPU.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 'dummy_doc.docx' for performance metrics using LLM...\n",
      "\n",
      "Processing Sentence: \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"The error rate was 0.01% in the last quarter.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Expected uptime is 99.9% for the server.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Accuracy improved to 95.5%.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Project budget was $1,250,000.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Client satisfaction improved by 20%.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "No performance metrics found in the entire document using the LLM.\n",
      "\n",
      "--- LLM-based Extraction Notes ---\n",
      "This approach leverages the LLM's understanding of instructions and context.\n",
      "Performance (speed and accuracy) will depend on:\n",
      "1. The chosen LLM model (smaller models are faster but less accurate).\n",
      "2. Your hardware (GPU is highly recommended for LLMs).\n",
      "3. The clarity and specificity of the prompt engineering.\n",
      "Errors in LLM output (e.g., malformed JSON) are possible and handled with error checks.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os \n",
    "\n",
    "# --- 1. Load the LLM for Extraction (no changes here as loading is successful) ---\n",
    "llm_pipeline = None\n",
    "\n",
    "try:\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "\n",
    "    # Determine device for optimal loading\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        dtype = torch.bfloat16\n",
    "        try:\n",
    "            _ = torch.randn(1, 1).to(device).to(torch.bfloat16)\n",
    "        except Exception:\n",
    "            print(\"Warning: bfloat16 not supported by GPU, falling back to float16.\")\n",
    "            dtype = torch.float16\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        dtype = torch.float32 # Use float32 on CPU for maximum compatibility\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=dtype,       # Use determined dtype\n",
    "        device_map=\"auto\" if device == \"cuda\" else None # Only use device_map for CUDA\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    llm_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(f\"Successfully loaded LLM: {model_name} on {device.upper()}.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM model '{model_name}': {e}\")\n",
    "    print(\"\\n--- Troubleshooting Tips for LLM Loading ---\")\n",
    "    print(\"1. **Check RAM/VRAM:** Ensure you have enough memory (e.g., 2GB+ for TinyLlama).\")\n",
    "    print(\"2. **Compatibility:** If `torch_dtype` or `device_map` cause issues, review the comments in the `try` block for `AutoModelForCausalLM.from_pretrained`.\")\n",
    "    print(\"3. **Corrupted Download:** If it persists, delete the model from your Hugging Face cache and try again.\")\n",
    "    print(f\"   (e.g., `huggingface-cli delete-cache {model_name}`)\")\n",
    "    llm_pipeline = None\n",
    "\n",
    "\n",
    "def extract_info_with_llm(sentence, llm_pipeline):\n",
    "    \"\"\"\n",
    "    Uses an LLM to extract performance keywords and their values from a sentence.\n",
    "    \"\"\"\n",
    "    if not llm_pipeline:\n",
    "        return []\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at extracting performance metrics from text. Extract keywords and their numerical values.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Extract performance keywords and their corresponding numerical values from the following sentence.\n",
    "        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\n",
    "        Output the information as a JSON list, where each item is an object with 'keyword' and 'value' fields. The value should be a number (integer or float).\n",
    "        If no performance keywords and values are found, output an empty JSON list [].\n",
    "\n",
    "        Sentence: \"{sentence}\"\n",
    "        Output:\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    try:\n",
    "        # --- THE FIX: Pass formatted_prompt as the 'inputs' keyword argument ---\n",
    "        generated_text = llm_pipeline(\n",
    "            inputs=formatted_prompt, # <--- THIS IS THE CHANGE\n",
    "            stop_sequence=[\"\\n\\n\", \"```\", \"\\nOutput:\", \"\\n[]\"]\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- LLM Raw Full Output (for debugging) ---\")\n",
    "        full_conversation_output = generated_text[0]['generated_text']\n",
    "        print(full_conversation_output) \n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "        llm_output = full_conversation_output.replace(formatted_prompt, \"\").strip()\n",
    "        \n",
    "        print(\"\\n--- Extracted LLM Response for Parsing ---\")\n",
    "        print(llm_output)\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "        # --- Enhanced JSON Parsing (remains the same) ---\n",
    "        json_code_block_match = re.search(r'```json\\s*(.*?)\\s*```', llm_output, re.DOTALL)\n",
    "        if json_code_block_match:\n",
    "            json_string_candidate = json_code_block_match.group(1)\n",
    "            print(\"Found JSON in code block.\") \n",
    "        else:\n",
    "            json_string_candidate = llm_output\n",
    "            print(\"No JSON code block found, trying direct parse.\") \n",
    "\n",
    "        json_start = json_string_candidate.find('[')\n",
    "        json_end = json_string_candidate.rfind(']')\n",
    "\n",
    "        if json_start != -1 and json_end != -1 and json_end > json_start:\n",
    "            json_string = json_string_candidate[json_start : json_end + 1]\n",
    "            json_string = json_string.strip()\n",
    "            print(f\"Attempting to parse JSON string: {json_string}\") \n",
    "\n",
    "            try:\n",
    "                extracted_data = json.loads(json_string)\n",
    "                if isinstance(extracted_data, list):\n",
    "                    valid_extractions = []\n",
    "                    for item in extracted_data:\n",
    "                        if isinstance(item, dict) and 'keyword' in item and 'value' in item:\n",
    "                            try:\n",
    "                                item['value'] = float(item['value'])\n",
    "                                if item['value'] == int(item['value']):\n",
    "                                    item['value'] = int(item['value'])\n",
    "                                valid_extractions.append(item)\n",
    "                            except ValueError:\n",
    "                                print(f\"Warning: Invalid value '{item.get('value')}' for keyword '{item.get('keyword')}'. Skipping.\") \n",
    "                                pass \n",
    "                    return valid_extractions\n",
    "                print(\"Warning: JSON parsed but not a list.\") \n",
    "                return []\n",
    "            except json.JSONDecodeError as jde:\n",
    "                print(f\"Warning: Could not parse JSON from LLM output: {jde}\")\n",
    "                print(f\"LLM Raw Output snippet causing error: {json_string_candidate[:200]}...\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"Warning: LLM did not output expected JSON structure ([...]). Raw output snippet: {llm_output[:200]}...\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during LLM inference: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Rest of your existing code (Document extraction, main, dummy doc creation) ---\n",
    "def extract_sentences_from_docx(file_path):\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|!)\\s+|(?<=\\.|\\?|!)\"?\\s+')\n",
    "    try:\n",
    "        document = Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in document.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        document_content = \"\\n\".join(full_text)\n",
    "        sentences = sentence_endings.split(document_content)\n",
    "        for sentence in sentences:\n",
    "            stripped_sentence = sentence.strip()\n",
    "            if stripped_sentence:\n",
    "                yield stripped_sentence\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Word document: {e}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    if not llm_pipeline:\n",
    "        print(\"LLM pipeline not initialized. Cannot perform LLM-based extraction.\")\n",
    "        return\n",
    "\n",
    "    dummy_doc_path = \"dummy_doc.docx\"\n",
    "    if not os.path.exists(dummy_doc_path):\n",
    "        print(f\"\\n--- Creating a dummy '{dummy_doc_path}' for testing ---\")\n",
    "        doc = Document()\n",
    "        doc.add_paragraph(\"The model achieved a recall of 0.92 and a precision of 0.88. Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\")\n",
    "        doc.add_paragraph(\"The error rate was 0.01% in the last quarter. Expected uptime is 99.9% for the server.\")\n",
    "        doc.add_paragraph(\"This sentence has no specific performance metrics. Accuracy improved to 95.5%.\")\n",
    "        doc.add_paragraph(\"Project budget was $1,250,000. Client satisfaction improved by 20%.\")\n",
    "        doc.save(dummy_doc_path)\n",
    "        print(f\"Dummy document created at: {dummy_doc_path}\")\n",
    "\n",
    "    file_path = dummy_doc_path \n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File does not exist at the specified path: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    if not file_path.lower().endswith('.docx'):\n",
    "        print(f\"Error: The provided file is not a .docx document. Please provide a Word document.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing document '{os.path.basename(file_path)}' for performance metrics using LLM...\")\n",
    "    found_any = False\n",
    "    \n",
    "    number_regex = re.compile(r'\\d') \n",
    "\n",
    "    for sentence in extract_sentences_from_docx(file_path):\n",
    "        if number_regex.search(sentence):\n",
    "            print(f\"\\nProcessing Sentence: \\\"{sentence}\\\"\")\n",
    "            extracted_metrics = extract_info_with_llm(sentence, llm_pipeline)\n",
    "            \n",
    "            if extracted_metrics:\n",
    "                found_any = True\n",
    "                print(\"Extracted Metrics:\")\n",
    "                for item in extracted_metrics:\n",
    "                    print(f\"  - Keyword: '{item['keyword']}', Value: {item['value']}\")\n",
    "                print(\"=\" * 70)\n",
    "            else:\n",
    "                print(\"No performance metrics extracted for this sentence.\")\n",
    "                print(\"=\" * 70)\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No performance metrics found in the entire document using the LLM.\")\n",
    "\n",
    "    print(\"\\n--- LLM-based Extraction Notes ---\")\n",
    "    print(\"This approach leverages the LLM's understanding of instructions and context.\")\n",
    "    print(\"Performance (speed and accuracy) will depend on:\")\n",
    "    print(\"1. The chosen LLM model (smaller models are faster but less accurate).\")\n",
    "    print(\"2. Your hardware (GPU is highly recommended for LLMs).\")\n",
    "    print(\"3. The clarity and specificity of the prompt engineering.\")\n",
    "    print(\"Errors in LLM output (e.g., malformed JSON) are possible and handled with error checks.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1f47f5d9-d0c3-4bac-aa5c-c038899acfe9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0 on CPU.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 'dummy_doc.docx' for performance metrics using LLM...\n",
      "\n",
      "Processing Sentence: \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"The error rate was 0.01% in the last quarter.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Expected uptime is 99.9% for the server.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Accuracy improved to 95.5%.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Project budget was $1,250,000.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Client satisfaction improved by 20%.\"\n",
      "An error occurred during LLM inference: TextEncodeInput must be Union[TextInputSequence, Tuple[InputSequence, InputSequence]]\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "No performance metrics found in the entire document using the LLM.\n",
      "\n",
      "--- LLM-based Extraction Notes ---\n",
      "This approach leverages the LLM's understanding of instructions and context.\n",
      "Performance (speed and accuracy) will depend on:\n",
      "1. The chosen LLM model (smaller models are faster but less accurate).\n",
      "2. Your hardware (GPU is highly recommended for LLMs).\n",
      "3. The clarity and specificity of the prompt engineering.\n",
      "Errors in LLM output (e.g., malformed JSON) are possible and handled with error checks.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os \n",
    "\n",
    "# --- 1. Load the LLM for Extraction (no changes here as loading is successful) ---\n",
    "llm_pipeline = None\n",
    "\n",
    "try:\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "\n",
    "    # Determine device for optimal loading\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        dtype = torch.bfloat16\n",
    "        try:\n",
    "            _ = torch.randn(1, 1).to(device).to(torch.bfloat16)\n",
    "        except Exception:\n",
    "            print(\"Warning: bfloat16 not supported by GPU, falling back to float16.\")\n",
    "            dtype = torch.float16\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        dtype = torch.float32 # Use float32 on CPU for maximum compatibility\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=dtype,       # Use determined dtype\n",
    "        device_map=\"auto\" if device == \"cuda\" else None # Only use device_map for CUDA\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    \n",
    "    llm_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(f\"Successfully loaded LLM: {model_name} on {device.upper()}.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM model '{model_name}': {e}\")\n",
    "    print(\"\\n--- Troubleshooting Tips for LLM Loading ---\")\n",
    "    print(\"1. **Check RAM/VRAM:** Ensure you have enough memory (e.g., 2GB+ for TinyLlama).\")\n",
    "    print(\"2. **Compatibility:** If `torch_dtype` or `device_map` cause issues, review the comments in the `try` block for `AutoModelForCausalLM.from_pretrained`.\")\n",
    "    print(\"3. **Corrupted Download:** If it persists, delete the model from your Hugging Face cache and try again.\")\n",
    "    print(f\"   (e.g., `huggingface-cli delete-cache {model_name}`)\")\n",
    "    llm_pipeline = None\n",
    "\n",
    "\n",
    "def extract_info_with_llm(sentence, llm_pipeline):\n",
    "    \"\"\"\n",
    "    Uses an LLM to extract performance keywords and their values from a sentence.\n",
    "    \"\"\"\n",
    "    if not llm_pipeline:\n",
    "        return []\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at extracting performance metrics from text. Extract keywords and their numerical values.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Extract performance keywords and their corresponding numerical values from the following sentence.\n",
    "        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\n",
    "        Output the information as a JSON list, where each item is an object with 'keyword' and 'value' fields. The value should be a number (integer or float).\n",
    "        If no performance keywords and values are found, output an empty JSON list [].\n",
    "\n",
    "        Sentence: \"{sentence}\"\n",
    "        Output:\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # Manually apply the chat template to get the exact string the model expects.\n",
    "    # `add_generation_prompt=True` adds the final token (e.g., \"<|assistant|>\\n\")\n",
    "    # that tells the model it's its turn to generate.\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    try:\n",
    "        # --- THE FINAL ATTEMPT AT THE FIX: Pass formatted_prompt as a positional argument ---\n",
    "        generated_text = llm_pipeline(\n",
    "            formatted_prompt, # <--- PASS THE STRING DIRECTLY AS POSITIONAL ARGUMENT\n",
    "            stop_sequence=[\"\\n\\n\", \"```\", \"\\nOutput:\", \"\\n[]\"]\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- LLM Raw Full Output (for debugging) ---\")\n",
    "        full_conversation_output = generated_text[0]['generated_text']\n",
    "        print(full_conversation_output) \n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "        # Now, `llm_output` needs to be extracted from the *end* of the conversation.\n",
    "        # It's everything after the `formatted_prompt` itself.\n",
    "        llm_output = full_conversation_output.replace(formatted_prompt, \"\").strip()\n",
    "        \n",
    "        print(\"\\n--- Extracted LLM Response for Parsing ---\")\n",
    "        print(llm_output)\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "        # --- Enhanced JSON Parsing (remains the same) ---\n",
    "        json_code_block_match = re.search(r'```json\\s*(.*?)\\s*```', llm_output, re.DOTALL)\n",
    "        if json_code_block_match:\n",
    "            json_string_candidate = json_code_block_match.group(1)\n",
    "            print(\"Found JSON in code block.\") \n",
    "        else:\n",
    "            json_string_candidate = llm_output\n",
    "            print(\"No JSON code block found, trying direct parse.\") \n",
    "\n",
    "        json_start = json_string_candidate.find('[')\n",
    "        json_end = json_string_candidate.rfind(']')\n",
    "\n",
    "        if json_start != -1 and json_end != -1 and json_end > json_start:\n",
    "            json_string = json_string_candidate[json_start : json_end + 1]\n",
    "            json_string = json_string.strip()\n",
    "            print(f\"Attempting to parse JSON string: {json_string}\") \n",
    "\n",
    "            try:\n",
    "                extracted_data = json.loads(json_string)\n",
    "                if isinstance(extracted_data, list):\n",
    "                    valid_extractions = []\n",
    "                    for item in extracted_data:\n",
    "                        if isinstance(item, dict) and 'keyword' in item and 'value' in item:\n",
    "                            try:\n",
    "                                item['value'] = float(item['value'])\n",
    "                                if item['value'] == int(item['value']):\n",
    "                                    item['value'] = int(item['value'])\n",
    "                                valid_extractions.append(item)\n",
    "                            except ValueError:\n",
    "                                print(f\"Warning: Invalid value '{item.get('value')}' for keyword '{item.get('keyword')}'. Skipping.\") \n",
    "                                pass \n",
    "                    return valid_extractions\n",
    "                print(\"Warning: JSON parsed but not a list.\") \n",
    "                return []\n",
    "            except json.JSONDecodeError as jde:\n",
    "                print(f\"Warning: Could not parse JSON from LLM output: {jde}\")\n",
    "                print(f\"LLM Raw Output snippet causing error: {json_string_candidate[:200]}...\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"Warning: LLM did not output expected JSON structure ([...]). Raw output snippet: {llm_output[:200]}...\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during LLM inference: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Rest of your existing code (Document extraction, main, dummy doc creation) ---\n",
    "def extract_sentences_from_docx(file_path):\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|!)\\s+|(?<=\\.|\\?|!)\"?\\s+')\n",
    "    try:\n",
    "        document = Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in document.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        document_content = \"\\n\".join(full_text)\n",
    "        sentences = sentence_endings.split(document_content)\n",
    "        for sentence in sentences:\n",
    "            stripped_sentence = sentence.strip()\n",
    "            if stripped_sentence:\n",
    "                yield stripped_sentence\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Word document: {e}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    if not llm_pipeline:\n",
    "        print(\"LLM pipeline not initialized. Cannot perform LLM-based extraction.\")\n",
    "        return\n",
    "\n",
    "    dummy_doc_path = \"dummy_doc.docx\"\n",
    "    if not os.path.exists(dummy_doc_path):\n",
    "        print(f\"\\n--- Creating a dummy '{dummy_doc_path}' for testing ---\")\n",
    "        doc = Document()\n",
    "        doc.add_paragraph(\"The model achieved a recall of 0.92 and a precision of 0.88. Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\")\n",
    "        doc.add_paragraph(\"The error rate was 0.01% in the last quarter. Expected uptime is 99.9% for the server.\")\n",
    "        doc.add_paragraph(\"This sentence has no specific performance metrics. Accuracy improved to 95.5%.\")\n",
    "        doc.add_paragraph(\"Project budget was $1,250,000. Client satisfaction improved by 20%.\")\n",
    "        doc.save(dummy_doc_path)\n",
    "        print(f\"Dummy document created at: {dummy_doc_path}\")\n",
    "\n",
    "    file_path = dummy_doc_path \n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File does not exist at the specified path: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    if not file_path.lower().endswith('.docx'):\n",
    "        print(f\"Error: The provided file is not a .docx document. Please provide a Word document.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing document '{os.path.basename(file_path)}' for performance metrics using LLM...\")\n",
    "    found_any = False\n",
    "    \n",
    "    number_regex = re.compile(r'\\d') \n",
    "\n",
    "    for sentence in extract_sentences_from_docx(file_path):\n",
    "        if number_regex.search(sentence):\n",
    "            print(f\"\\nProcessing Sentence: \\\"{sentence}\\\"\")\n",
    "            extracted_metrics = extract_info_with_llm(sentence, llm_pipeline)\n",
    "            \n",
    "            if extracted_metrics:\n",
    "                found_any = True\n",
    "                print(\"Extracted Metrics:\")\n",
    "                for item in extracted_metrics:\n",
    "                    print(f\"  - Keyword: '{item['keyword']}', Value: {item['value']}\")\n",
    "                print(\"=\" * 70)\n",
    "            else:\n",
    "                print(\"No performance metrics extracted for this sentence.\")\n",
    "                print(\"=\" * 70)\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No performance metrics found in the entire document using the LLM.\")\n",
    "\n",
    "    print(\"\\n--- LLM-based Extraction Notes ---\")\n",
    "    print(\"This approach leverages the LLM's understanding of instructions and context.\")\n",
    "    print(\"Performance (speed and accuracy) will depend on:\")\n",
    "    print(\"1. The chosen LLM model (smaller models are faster but less accurate).\")\n",
    "    print(\"2. Your hardware (GPU is highly recommended for LLMs).\")\n",
    "    print(\"3. The clarity and specificity of the prompt engineering.\")\n",
    "    print(\"Errors in LLM output (e.g., malformed JSON) are possible and handled with error checks.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fb7ae795-5d0b-4fc9-9524-bef45cd41474",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully loaded LLM: TinyLlama/TinyLlama-1.1B-Chat-v1.0 on CPU.\n",
      "--------------------------------------------------\n",
      "\n",
      "Processing document 'dummy_doc.docx' for performance metrics using LLM...\n",
      "\n",
      "Processing Sentence: \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"The error rate was 0.01% in the last quarter.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Expected uptime is 99.9% for the server.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Accuracy improved to 95.5%.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Project budget was $1,250,000.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "\n",
      "Processing Sentence: \"Client satisfaction improved by 20%.\"\n",
      "An error occurred during LLM inference: __call__() missing 1 required positional argument: 'text_inputs'\n",
      "No performance metrics extracted for this sentence.\n",
      "======================================================================\n",
      "No performance metrics found in the entire document using the LLM.\n",
      "\n",
      "--- LLM-based Extraction Notes ---\n",
      "This approach leverages the LLM's understanding of instructions and context.\n",
      "Performance (speed and accuracy) will depend on:\n",
      "1. The chosen LLM model (smaller models are faster but less accurate).\n",
      "2. Your hardware (GPU is highly recommended for LLMs).\n",
      "3. The clarity and specificity of the prompt engineering.\n",
      "Errors in LLM output (e.g., malformed JSON) are possible and handled with error checks.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "from docx import Document\n",
    "from transformers import pipeline, AutoTokenizer, AutoModelForCausalLM\n",
    "import torch\n",
    "import json\n",
    "import os \n",
    "\n",
    "# --- 1. Load the LLM for Extraction (no changes here as loading is successful) ---\n",
    "llm_pipeline = None\n",
    "\n",
    "try:\n",
    "    model_name = \"TinyLlama/TinyLlama-1.1B-Chat-v1.0\" \n",
    "\n",
    "    # Determine device for optimal loading\n",
    "    if torch.cuda.is_available():\n",
    "        device = \"cuda\"\n",
    "        dtype = torch.bfloat16\n",
    "        try:\n",
    "            _ = torch.randn(1, 1).to(device).to(torch.bfloat16)\n",
    "        except Exception:\n",
    "            print(\"Warning: bfloat16 not supported by GPU, falling back to float16.\")\n",
    "            dtype = torch.float16\n",
    "    else:\n",
    "        device = \"cpu\"\n",
    "        dtype = torch.float32 # Use float32 on CPU for maximum compatibility\n",
    "    print(f\"Device set to use {device}\") # Add this to confirm device\n",
    "\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_name,\n",
    "        torch_dtype=dtype,       # Use determined dtype\n",
    "        device_map=\"auto\" if device == \"cuda\" else None # Only use device_map for CUDA\n",
    "    )\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    # Important: Set pad_token_id for generation, especially if batching or varying lengths\n",
    "    if tokenizer.pad_token_id is None:\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "        print(\"Set tokenizer.pad_token_id to tokenizer.eos_token_id\")\n",
    "    \n",
    "    llm_pipeline = pipeline(\n",
    "        \"text-generation\",\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        max_new_tokens=100,\n",
    "        do_sample=False,\n",
    "        temperature=0.0,\n",
    "        top_p=0.9,\n",
    "    )\n",
    "    print(f\"Successfully loaded LLM: {model_name} on {device.upper()}.\")\n",
    "    print(\"-\" * 50)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Error loading LLM model '{model_name}': {e}\")\n",
    "    print(\"\\n--- Troubleshooting Tips for LLM Loading ---\")\n",
    "    print(\"1. **Check RAM/VRAM:** Ensure you have enough memory (e.g., 2GB+ for TinyLlama).\")\n",
    "    print(\"2. **Compatibility:** If `torch_dtype` or `device_map` cause issues, review the comments in the `try` block for `AutoModelForCausalLM.from_pretrained`.\")\n",
    "    print(\"3. **Corrupted Download:** If it persists, delete the model from your Hugging Face cache and try again.\")\n",
    "    print(f\"   (e.g., `huggingface-cli delete-cache {model_name}`)\")\n",
    "    llm_pipeline = None\n",
    "\n",
    "\n",
    "def extract_info_with_llm(sentence, llm_pipeline):\n",
    "    \"\"\"\n",
    "    Uses an LLM to extract performance keywords and their values from a sentence.\n",
    "    \"\"\"\n",
    "    if not llm_pipeline:\n",
    "        return []\n",
    "\n",
    "    messages = [\n",
    "        {\"role\": \"system\", \"content\": \"You are an expert at extracting performance metrics from text. Extract keywords and their numerical values.\"},\n",
    "        {\"role\": \"user\", \"content\": f\"\"\"Extract performance keywords and their corresponding numerical values from the following sentence.\n",
    "        Examples of performance keywords include \"recall\", \"precision\", \"accuracy\", \"f1-score\", \"throughput\", \"latency\", \"response time\", \"error rate\", \"conversion rate\", \"uptime\", etc.\n",
    "        Output the information as a JSON list, where each item is an object with 'keyword' and 'value' fields. The value should be a number (integer or float).\n",
    "        If no performance keywords and values are found, output an empty JSON list [].\n",
    "\n",
    "        Sentence: \"{sentence}\"\n",
    "        Output:\n",
    "        \"\"\"}\n",
    "    ]\n",
    "\n",
    "    # Manually apply the chat template to get the exact string.\n",
    "    formatted_prompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    \n",
    "    try:\n",
    "        # --- THE NEW ATTEMPT AT THE FIX: Tokenize manually and pass inputs_ids ---\n",
    "        # This explicitly prepares the input as numerical IDs, which is what the model's\n",
    "        # underlying forward pass ultimately expects.\n",
    "        \n",
    "        # Tokenize the prompt\n",
    "        inputs = tokenizer(formatted_prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "        # Move inputs to the correct device (CPU in your case)\n",
    "        inputs = {k: v.to(model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # Now call the pipeline with the tokenized inputs.\n",
    "        # This forces the pipeline to use your pre-tokenized input,\n",
    "        # bypassing its internal tokenization logic that might be causing the error.\n",
    "        generated_text = llm_pipeline(\n",
    "            **inputs, # Unpack the dictionary of input_ids and attention_mask\n",
    "            stop_sequence=[\"\\n\\n\", \"```\", \"\\nOutput:\", \"\\n[]\"],\n",
    "            # Ensure the max_new_tokens isn't cutting off the prompt prematurely\n",
    "            max_new_tokens=100, # Re-confirm this as it might interact\n",
    "            pad_token_id=tokenizer.pad_token_id # Explicitly pass pad_token_id\n",
    "        )\n",
    "        \n",
    "        print(\"\\n--- LLM Raw Full Output (for debugging) ---\")\n",
    "        full_conversation_output = generated_text[0]['generated_text']\n",
    "        print(full_conversation_output) \n",
    "        print(\"---------------------------------------------\")\n",
    "\n",
    "        # Now, `llm_output` needs to be extracted from the *end* of the conversation.\n",
    "        # It's everything after the `formatted_prompt` itself.\n",
    "        llm_output = full_conversation_output.replace(formatted_prompt, \"\").strip()\n",
    "        \n",
    "        print(\"\\n--- Extracted LLM Response for Parsing ---\")\n",
    "        print(llm_output)\n",
    "        print(\"------------------------------------------\")\n",
    "\n",
    "        # --- Enhanced JSON Parsing (remains the same) ---\n",
    "        json_code_block_match = re.search(r'```json\\s*(.*?)\\s*```', llm_output, re.DOTALL)\n",
    "        if json_code_block_match:\n",
    "            json_string_candidate = json_code_block_match.group(1)\n",
    "            print(\"Found JSON in code block.\") \n",
    "        else:\n",
    "            json_string_candidate = llm_output\n",
    "            print(\"No JSON code block found, trying direct parse.\") \n",
    "\n",
    "        json_start = json_string_candidate.find('[')\n",
    "        json_end = json_string_candidate.rfind(']')\n",
    "\n",
    "        if json_start != -1 and json_end != -1 and json_end > json_start:\n",
    "            json_string = json_string_candidate[json_start : json_end + 1]\n",
    "            json_string = json_string.strip()\n",
    "            print(f\"Attempting to parse JSON string: {json_string}\") \n",
    "\n",
    "            try:\n",
    "                extracted_data = json.loads(json_string)\n",
    "                if isinstance(extracted_data, list):\n",
    "                    valid_extractions = []\n",
    "                    for item in extracted_data:\n",
    "                        if isinstance(item, dict) and 'keyword' in item and 'value' in item:\n",
    "                            try:\n",
    "                                item['value'] = float(item['value'])\n",
    "                                if item['value'] == int(item['value']):\n",
    "                                    item['value'] = int(item['value'])\n",
    "                                valid_extractions.append(item)\n",
    "                            except ValueError:\n",
    "                                print(f\"Warning: Invalid value '{item.get('value')}' for keyword '{item.get('keyword')}'. Skipping.\") \n",
    "                                pass \n",
    "                    return valid_extractions\n",
    "                print(\"Warning: JSON parsed but not a list.\") \n",
    "                return []\n",
    "            except json.JSONDecodeError as jde:\n",
    "                print(f\"Warning: Could not parse JSON from LLM output: {jde}\")\n",
    "                print(f\"LLM Raw Output snippet causing error: {json_string_candidate[:200]}...\")\n",
    "                return []\n",
    "        else:\n",
    "            print(f\"Warning: LLM did not output expected JSON structure ([...]). Raw output snippet: {llm_output[:200]}...\")\n",
    "            return []\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred during LLM inference: {e}\")\n",
    "        return []\n",
    "\n",
    "# --- Rest of your existing code (Document extraction, main, dummy doc creation) ---\n",
    "def extract_sentences_from_docx(file_path):\n",
    "    sentence_endings = re.compile(r'(?<!\\w\\.\\w.)(?<![A-Z][a-z]\\.)(?<![A-Z]\\.)(?<=\\.|\\?|!)\\s+|(?<=\\.|\\?|!)\"?\\s+')\n",
    "    try:\n",
    "        document = Document(file_path)\n",
    "        full_text = []\n",
    "        for paragraph in document.paragraphs:\n",
    "            full_text.append(paragraph.text)\n",
    "        document_content = \"\\n\".join(full_text)\n",
    "        sentences = sentence_endings.split(document_content)\n",
    "        for sentence in sentences:\n",
    "            stripped_sentence = sentence.strip()\n",
    "            if stripped_sentence:\n",
    "                yield stripped_sentence\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: File not found at {file_path}\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while reading the Word document: {e}\")\n",
    "        return\n",
    "\n",
    "def main():\n",
    "    if not llm_pipeline:\n",
    "        print(\"LLM pipeline not initialized. Cannot perform LLM-based extraction.\")\n",
    "        return\n",
    "\n",
    "    dummy_doc_path = \"dummy_doc.docx\"\n",
    "    if not os.path.exists(dummy_doc_path):\n",
    "        print(f\"\\n--- Creating a dummy '{dummy_doc_path}' for testing ---\")\n",
    "        doc = Document()\n",
    "        doc.add_paragraph(\"The model achieved a recall of 0.92 and a precision of 0.88. Our system's throughput reached 1200 requests per second, with a latency of only 50 milliseconds.\")\n",
    "        doc.add_paragraph(\"The error rate was 0.01% in the last quarter. Expected uptime is 99.9% for the server.\")\n",
    "        doc.add_paragraph(\"This sentence has no specific performance metrics. Accuracy improved to 95.5%.\")\n",
    "        doc.add_paragraph(\"Project budget was $1,250,000. Client satisfaction improved by 20%.\")\n",
    "        doc.save(dummy_doc_path)\n",
    "        print(f\"Dummy document created at: {dummy_doc_path}\")\n",
    "\n",
    "    file_path = dummy_doc_path \n",
    "\n",
    "    if not os.path.exists(file_path):\n",
    "        print(f\"Error: File does not exist at the specified path: {file_path}\")\n",
    "        return\n",
    "    \n",
    "    if not file_path.lower().endswith('.docx'):\n",
    "        print(f\"Error: The provided file is not a .docx document. Please provide a Word document.\")\n",
    "        return\n",
    "\n",
    "    print(f\"\\nProcessing document '{os.path.basename(file_path)}' for performance metrics using LLM...\")\n",
    "    found_any = False\n",
    "    \n",
    "    number_regex = re.compile(r'\\d') \n",
    "\n",
    "    for sentence in extract_sentences_from_docx(file_path):\n",
    "        if number_regex.search(sentence):\n",
    "            print(f\"\\nProcessing Sentence: \\\"{sentence}\\\"\")\n",
    "            extracted_metrics = extract_info_with_llm(sentence, llm_pipeline)\n",
    "            \n",
    "            if extracted_metrics:\n",
    "                found_any = True\n",
    "                print(\"Extracted Metrics:\")\n",
    "                for item in extracted_metrics:\n",
    "                    print(f\"  - Keyword: '{item['keyword']}', Value: {item['value']}\")\n",
    "                print(\"=\" * 70)\n",
    "            else:\n",
    "                print(\"No performance metrics extracted for this sentence.\")\n",
    "                print(\"=\" * 70)\n",
    "\n",
    "    if not found_any:\n",
    "        print(\"No performance metrics found in the entire document using the LLM.\")\n",
    "\n",
    "    print(\"\\n--- LLM-based Extraction Notes ---\")\n",
    "    print(\"This approach leverages the LLM's understanding of instructions and context.\")\n",
    "    print(\"Performance (speed and accuracy) will depend on:\")\n",
    "    print(\"1. The chosen LLM model (smaller models are faster but less accurate).\")\n",
    "    print(\"2. Your hardware (GPU is highly recommended for LLMs).\")\n",
    "    print(\"3. The clarity and specificity of the prompt engineering.\")\n",
    "    print(\"Errors in LLM output (e.g., malformed JSON) are possible and handled with error checks.\")\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8684c5-65ad-48c3-9420-9f09f011920b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c9e25a4-a5f4-4222-92f1-ef83436dae0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9aa5c778-fc41-44cb-9f63-2629f8cbcb4f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1685748-f355-4935-b913-819a822e3ff9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c620d80-626e-46f8-802a-de9cd5218630",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6b143595-4bda-4520-8d9c-f7782e58da4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: The model achieved a recall of 0.92 and a precision of 0.88.\n",
      "Extracted: {}\n",
      "\n",
      "Sentence: Accuracy improved to 95.5%.\n",
      "Extracted: {}\n",
      "\n",
      "Sentence: F1-score was 0.81 while ROC AUC reached 0.93.\n",
      "Extracted: {}\n",
      "\n",
      "Sentence: RMSE is 3.5 and R2 score is 0.89.\n",
      "Extracted: {}\n",
      "\n",
      "Sentence: The models sensitivity dropped to 74.2% while specificity remained at 89%.\n",
      "Extracted: {}\n",
      "\n",
      "Sentence: Area under curve is around 91.7 percent with a mean absolute error of 1.2.\n",
      "Extracted: {}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Load a small, instruction-tuned model\n",
    "model_name = \"google/flan-t5-base\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\"text2text-generation\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "def extract_metrics_with_flan(sentences):\n",
    "    results = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        prompt = f\"\"\"Extract all performance metrics and their numeric values from the following sentence. Return as JSON with metric names as keys and values as floats. Convert percentages to float (e.g., 95.5%  95.5).\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = generator(prompt, max_new_tokens=128)[0][\"generated_text\"]\n",
    "\n",
    "            # Try to extract a valid JSON object from the response\n",
    "            match = re.search(r\"\\{.*\\}\", response, re.DOTALL)\n",
    "            if match:\n",
    "                result = json.loads(match.group())\n",
    "            else:\n",
    "                result = {}\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            result = {}\n",
    "\n",
    "        results.append(result)\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Example usage ---\n",
    "sentences = [\n",
    "    \"The model achieved a recall of 0.92 and a precision of 0.88.\",\n",
    "    \"Accuracy improved to 95.5%.\",\n",
    "    \"F1-score was 0.81 while ROC AUC reached 0.93.\",\n",
    "    \"RMSE is 3.5 and R2 score is 0.89.\",\n",
    "    \"The models sensitivity dropped to 74.2% while specificity remained at 89%.\",\n",
    "    \"Area under curve is around 91.7 percent with a mean absolute error of 1.2.\"\n",
    "]\n",
    "\n",
    "results = extract_metrics_with_flan(sentences)\n",
    "for sent, metrics in zip(sentences, results):\n",
    "    print(f\"Sentence: {sent}\\nExtracted: {metrics}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7410c4c-88e9-42d4-bfba-bc640adbae57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import re\n",
    "import json\n",
    "\n",
    "# Use Phi-2 with the correct class\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=128)\n",
    "\n",
    "def extract_metric_names(sentences):\n",
    "    results = []\n",
    "\n",
    "    for sentence in sentences:\n",
    "        prompt = f\"\"\"Extract only the names of performance metrics mentioned in the following sentence. Ignore values. Return the result as a Python list of strings.\n",
    "\n",
    "Sentence: \"{sentence}\"\n",
    "\"\"\"\n",
    "\n",
    "        try:\n",
    "            response = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "\n",
    "            # Extract Python-style list\n",
    "            match = re.search(r\"\\[.*?\\]\", response, re.DOTALL)\n",
    "            if match:\n",
    "                result = eval(match.group(), {\"__builtins__\": None}, {})\n",
    "                if isinstance(result, list):\n",
    "                    results.append(result)\n",
    "                else:\n",
    "                    results.append([])\n",
    "            else:\n",
    "                results.append([])\n",
    "        except Exception as e:\n",
    "            print(f\"Error: {e}\")\n",
    "            results.append([])\n",
    "\n",
    "    return results\n",
    "\n",
    "# --- Example Usage ---\n",
    "sentences = [\n",
    "    \"The model achieved a recall of 0.92 and a precision of 0.88.\",\n",
    "    \"Accuracy improved to 95.5%.\",\n",
    "    \"F1-score was 0.81 while ROC AUC reached 0.93.\",\n",
    "    \"RMSE is 3.5 and R2 score is 0.89.\",\n",
    "    \"The models sensitivity dropped to 74.2% while specificity remained at 89%.\",\n",
    "    \"Area under curve is around 91.7 percent with a mean absolute error of 1.2.\"\n",
    "]\n",
    "\n",
    "results = extract_metric_names(sentences)\n",
    "for sent, metrics in zip(sentences, results):\n",
    "    print(f\"Sentence: {sent}\\nExtracted Metric Names: {metrics}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "861793e9-a4bc-4561-bc1e-12b1bf7c97d9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "877f5790-962e-4feb-9bc0-46831864a89c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd1600a-1bf3-4f04-9d1c-840b8a6406e7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d3fd1d3f-a288-4a90-8dc2-d08514e9416c",
   "metadata": {},
   "source": [
    "### Here, restart the kernel... Following 2 Cell Takes ***TREMENDOUSLY*** long to execute"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16751e1e-f456-4353-b1b9-87569e05bd27",
   "metadata": {},
   "source": [
    "###  Example Prompt with Few-Shot Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dd4afab3-6c8f-45b5-bcce-490bafa1a972",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Extract only the names of performance metrics mentioned in each sentence. Ignore values. \n",
    "Return the result as a Python list of strings.\n",
    "\n",
    "Example 1:\n",
    "Sentence: \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n",
    "Output: [\"recall\", \"precision\"]\n",
    "\n",
    "Example 2:\n",
    "Sentence: \"Accuracy improved to 95.5%.\"\n",
    "Output: [\"accuracy\"]\n",
    "\n",
    "Example 3:\n",
    "Sentence: \"F1-score was 0.81 while ROC AUC reached 0.93.\"\n",
    "Output: [\"f1-score\", \"roc auc\"]\n",
    "\n",
    "Now extract from this sentence:\n",
    "Sentence: \"The models sensitivity dropped to 74.2% while specificity remained at 89%.\"\n",
    "Output:\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cbd122ee-6581-4e8f-9424-969eeae1f740",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b1b49ac32f4c4f44bd5b1502727506ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "import re, json\n",
    "\n",
    "#model_name = \"microsoft/phi-2\"\n",
    "model_name = \"microsoft/phi-2\"\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name)\n",
    "\n",
    "generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=64)\n",
    "\n",
    "few_shot_prefix = \"\"\"\n",
    "Extract only the names of performance metrics mentioned in each sentence. Ignore values. \n",
    "Return the result as a Python list of strings.\n",
    "\n",
    "Example 1:\n",
    "Sentence: \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n",
    "Output: [\"recall\", \"precision\"]\n",
    "\n",
    "Example 2:\n",
    "Sentence: \"Accuracy improved to 95.5%.\"\n",
    "Output: [\"accuracy\"]\n",
    "\n",
    "Example 3:\n",
    "Sentence: \"F1-score was 0.81 while ROC AUC reached 0.93.\"\n",
    "Output: [\"f1-score\", \"roc auc\"]\n",
    "\n",
    "Now extract from this sentence:\n",
    "\"\"\"\n",
    "\n",
    "def extract_metric_names_fewshot(sentences):\n",
    "    results = []\n",
    "    for sentence in sentences:\n",
    "        prompt = few_shot_prefix + f'Sentence: \"{sentence}\"\\nOutput:'\n",
    "\n",
    "        try:\n",
    "            output = generator(prompt, do_sample=False)[0][\"generated_text\"]\n",
    "            match = re.search(r\"\\[.*?\\]\", output)\n",
    "            result = eval(match.group(), {\"__builtins__\": None}, {}) if match else []\n",
    "        except:\n",
    "            result = []\n",
    "        results.append(result)\n",
    "    return results\n",
    "\n",
    "# --- Example ---\n",
    "sentences = [\n",
    "    \"The models sensitivity dropped to 74.2% while specificity remained at 89%.\",\n",
    "    \"Area under curve is around 91.7 percent with a mean absolute error of 1.2.\"\n",
    "]\n",
    "\n",
    "results = extract_metric_names_fewshot(sentences)\n",
    "for s, r in zip(sentences, results):\n",
    "    print(f\"Sentence: {s}\\nExtracted Metric Names: {r}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98aea09d-86ac-4a91-bfbe-cf1bd55d62d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21ef77e-4576-4418-85f1-2e4428fcc95e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c219dc-78b3-4859-b00c-a664fc1db1d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "242d3317-537f-420c-98a6-7fb48ee45e38",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6781d880-2133-45a6-8986-e1dfdf94ccd7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b686148-803a-4725-a1dd-f26b21f335c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "\n",
    "transformers.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cce2f3a-2553-4a3f-9326-381a7ca1b99a",
   "metadata": {},
   "source": [
    "1. Choose a Base Model\n",
    "Recommended small LLMs:\n",
    "\n",
    "google/flan-t5-small or flan-t5-base (Seq2Seq, good for summarization)\n",
    "\n",
    "MBZUAI/LaMini-Flan-T5-783M (instruction-tuned, compact)\n",
    "\n",
    "microsoft/phi-2 (causal LM, use for generation, not summarization directly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16037ef7-d958-445f-8c40-8b64d859fcbd",
   "metadata": {},
   "source": [
    "2. Prepare Your Dataset\n",
    "For summarization:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "{\"text\": \"Long text here...\", \"summary\": \"Concise summary.\"}\n",
    "For text generation:\n",
    "\n",
    "json\n",
    "Copy\n",
    "Edit\n",
    "{\"prompt\": \"Write a poem about stars.\", \"output\": \"Stars shine bright in the night...\"}\n",
    "Format: Use Hugging Face datasets library or CSV/JSONL."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68221bb6-a7f2-44c0-8b7c-429888ed7d4b",
   "metadata": {},
   "source": [
    "3. Fine-Tune (with or without LoRA)\n",
    " Option A: Full Fine-Tuning (simple, but heavy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "081eb374-abbc-49d5-a868-3164ab04a37a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Seq2SeqTrainer, Seq2SeqTrainingArguments, DataCollatorForSeq2Seq\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "# Load your dataset\n",
    "dataset = load_dataset(\"json\", data_files={\"train\": \"performance_metrics_dataset.json\"})\n",
    "\n",
    "# Tokenize\n",
    "def preprocess(example):\n",
    "    input_text = example[\"text\"]\n",
    "    target_text = example[\"summary\"]\n",
    "    model_inputs = tokenizer(input_text, max_length=512, truncation=True)\n",
    "    labels = tokenizer(target_text, max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "tokenized = dataset.map(preprocess, batched=True)\n",
    "\n",
    "# Training setup\n",
    "args = Seq2SeqTrainingArguments(\n",
    "    output_dir=\"./flan-summarizer\",\n",
    "    #evaluation_strategy=\"no\",\n",
    "    per_device_train_batch_size=8,\n",
    "    num_train_epochs=3,\n",
    "    save_steps=500,\n",
    "    save_total_limit=2,\n",
    "    fp16=False,  # Set True if using GPU with mixed precision\n",
    ")\n",
    "\n",
    "trainer = Seq2SeqTrainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized[\"train\"],\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=DataCollatorForSeq2Seq(tokenizer, model=model),\n",
    ")\n",
    "\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a33e103-6789-41e6-ae5c-961a5fe94fd9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "10688bbb-846f-4d13-9a8e-a9cae528d494",
   "metadata": {},
   "source": [
    " Option B: Parameter-Efficient Tuning with LoRA (Recommended for low-resource)\n",
    "Use peft + trl + transformers (works great with T5, FLAN, etc.).\n",
    "\n",
    "bash\n",
    "Copy\n",
    "Edit\n",
    "pip install peft transformers accelerate datasets trl\n",
    "Would you like a full LoRA script template?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4cece90d-4c5d-4e93-8c4d-77d0fe2dc5f0",
   "metadata": {},
   "source": [
    "4. Inference After Fine-Tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaaa7b61-6e1b-48e4-a6cf-9eef8b3bf05e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "checkpoint_path = \"./flan-summarizer/checkpoint-75\"  # use actual checkpoint path\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")  # or use checkpoint_path if tokenizer was saved too\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(checkpoint_path)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Test\n",
    "text = \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n",
    "print(summarizer(text, max_length = 4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4e0990c-eeee-4326-ae1b-52a3eff2d299",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the test dataset\n",
    "with open(\"test_performance_metrics_dataset.json\", \"r\") as f:\n",
    "    test_data = json.load(f)\n",
    "\n",
    "    \n",
    "texts = [sample[\"text\"] for sample in test_data]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd050efd-07b4-4364-beeb-a1a99ca7406d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "# Load the model (adjust path as needed)\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"google/flan-t5-base\")\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(\"./flan-summarizer/checkpoint-75\")\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "# Run inference\n",
    "predictions = [summarizer(text)[0][\"summary_text\"] for text in texts]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f697128d-14a7-46c9-ba05-eaa126766bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, sample in enumerate(test_data[:5]):  # show first 5 examples\n",
    "    print(\"Input:\", sample[\"text\"])\n",
    "    print(\"Expected:\", sample[\"summary\"])\n",
    "    print(\"Predicted:\", predictions[i])\n",
    "    print(\"-\" * 50)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7454e836-0062-4d30-ba0d-cf96f78bde88",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcae87b-2d24-459c-a076-9fe2bfab8320",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d55a0e37-49cb-47ef-8fad-0d26a7e3601b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e67d9fa-b92a-4a43-82ae-8763e6a6e347",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a5c982d-f45f-4c35-8f09-f0a9ba6167aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "\n",
    "model_name = \"google/flan-t5-base\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "\n",
    "summarizer = pipeline(\"summarization\", model=model, tokenizer=tokenizer)\n",
    "\n",
    "text = \"The model achieved a recall of 0.92 and a precision of 0.88.\"\n",
    "print(summarizer(text, max_length = 5))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21831ee2-e4f5-4772-a734-9db16055f593",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a13de2e-1132-4724-95c5-df3d502414e8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b415ab0b-2970-4796-87f6-936eb2152428",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "65e4af15-ed3f-481d-b60e-67e568b76080",
   "metadata": {},
   "source": [
    "Consider you are an expert in python and LLM fine-tuning. Provide code to fine-tune the NER LLM model. Keep in mind the NER should tag the performance metrics keyword as \"PERF_KEY\". This list should be be dynamic and should not be hardcoded"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c03f3f6-2763-4add-b37f-3f16cf6b921c",
   "metadata": {},
   "source": [
    "1.  Install Required Packages (if not already installed)\n",
    "   pip install transformers datasets seqeval\n",
    "\n",
    "2.  Prepare the Training Script"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8aed34-b312-4e8f-b63f-ed855306f92c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# Example data - Replace with your own dataset\n",
    "examples = [\n",
    "    {\n",
    "        \"tokens\": [\"The\", \"model\", \"achieved\", \"a\", \"recall\", \"of\", \"0.92\", \"and\", \"precision\", \"of\", \"0.88\", \".\"],\n",
    "        \"ner_tags\": [\"O\", \"O\", \"O\", \"O\", \"B-PERF_KEY\", \"O\", \"O\", \"O\", \"B-PERF_KEY\", \"O\", \"O\", \"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Accuracy\", \"improved\", \"to\", \"95.5\", \"%\", \".\"],\n",
    "        \"ner_tags\": [\"B-PERF_KEY\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"The\", \"results\", \"are\", \"under\", \"review\", \".\"],\n",
    "        \"ner_tags\": [\"O\", \"O\", \"O\", \"O\", \"O\", \"O\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# Create Hugging Face dataset\n",
    "dataset = Dataset.from_list(examples)\n",
    "label_list = [\"O\", \"B-PERF_KEY\"]\n",
    "\n",
    "# Tokenizer and model\n",
    "model_checkpoint = \"distilbert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            label_ids.append(example[\"ner_tags\"][word_idx] if True else -100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list))\n",
    "\n",
    "# Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner_perf_key_model\",\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# Use seqeval metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# Trainer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3491e379-440a-4b34-b6c6-21292d62e79e",
   "metadata": {},
   "source": [
    "The error you're encountering is due to trying to store a Python object ('O')likely a non-integer label like a stringinto a column that is expected to be an integer (specifically, the \"labels\" field in your dataset). The Hugging Face datasets library (which uses Apache Arrow under the hood) is strict about data types.\n",
    "\n",
    "Heres how to fix it:\n",
    "\n",
    " The Problem\n",
    "In your tokenize_and_align_labels function, you're likely assigning labels that are still strings (like \"O\", \"B-LOC\", etc.) rather than their corresponding integer IDs.\n",
    "\n",
    "\n",
    "\n",
    " The Fix\n",
    "You must convert each label to its integer ID using the label_to_id mapping from your model config or dataset.\n",
    "\n",
    " Solution  Ensure Proper Label Mapping\n",
    "Heres how to properly handle this in tokenize_and_align_labels:\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label = example[\"ner_tags\"][word_idx]  # Assuming labels are already IDs\n",
    "            labels.append(label)\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "\n",
    "\n",
    " If example[\"ner_tags\"] are strings, you need to map them:\n",
    "label2id = {'O': 0, 'B-LOC': 1, 'I-LOC': 2, ...}  # example\n",
    "label = label2id[example[\"ner_tags\"][word_idx]]\n",
    "\n",
    "\n",
    "\n",
    " What To Do Next\n",
    "Inspect your ner_tags column:\n",
    "print(dataset[0][\"ner_tags\"])  # Are these strings or integers?\n",
    "\n",
    "\n",
    "If they are strings, convert them using label2id.\n",
    "Retry the mapping step:\n",
    "    tokenized_dataset = dataset.map(tokenize_and_align_labels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b682da0-7fd8-41a6-a165-0a6371c433e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# 1. Example data: replace with your actual dataset\n",
    "data = {\n",
    "    \"tokens\": [\n",
    "        [\"The\", \"model\", \"achieved\", \"95\", \"%\", \"accuracy\", \"on\", \"CIFAR-10\"],\n",
    "        [\"Precision\", \"was\", \"around\", \"90\", \"%\"]\n",
    "    ],\n",
    "    \"ner_tags\": [\n",
    "        [\"O\", \"O\", \"O\", \"B-PERF_VAL\", \"I-PERF_VAL\", \"B-PERF_KEY\", \"O\", \"O\"],\n",
    "        [\"B-PERF_KEY\", \"O\", \"O\", \"B-PERF_VAL\", \"I-PERF_VAL\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "\n",
    "data = {\n",
    "    \"tokens\": [\n",
    "        [\"The\", \"model\", \"achieved\", \"95\", \"%\", \"accuracy\", \"on\", \"CIFAR-10\"],\n",
    "        [\"Precision\", \"was\", \"around\", \"90\", \"%\"]\n",
    "    ],\n",
    "    \"ner_tags\": [\n",
    "        [\"O\", \"O\", \"O\", \"O\", \"O\", \"B-PERF_KEY\", \"O\", \"O\"],\n",
    "        [\"B-PERF_KEY\", \"O\", \"O\", \"O\", \"O\"]\n",
    "    ]\n",
    "}\n",
    "\n",
    "dataset = Dataset.from_dict(data)\n",
    "\n",
    "# 2. Define label list and mapping\n",
    "label_list = sorted(list(set(tag for seq in data[\"ner_tags\"] for tag in seq)))\n",
    "label2id = {label: i for i, label in enumerate(label_list)}\n",
    "id2label = {i: label for label, i in label2id.items()}\n",
    "\n",
    "# 3. Convert string labels to integer IDs in dataset\n",
    "def convert_tags_to_ids(example):\n",
    "    example[\"ner_tags\"] = [label2id[label] for label in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(convert_tags_to_ids)\n",
    "\n",
    "# 4. Tokenizer & Model\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id2label,\n",
    "    label2id=label2id\n",
    ")\n",
    "\n",
    "# 5. Tokenize & align labels\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    labels = []\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            labels.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            labels.append(example[\"ner_tags\"][word_idx])\n",
    "        else:\n",
    "            labels.append(-100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# 6. Data Collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# 7. Evaluation metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [id2label[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [id2label[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    return seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "\n",
    "# 8. Training Arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model_perf_key\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 9. Trainer\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 10. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68b979ad-a984-4735-9e06-e67103646f56",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0598422f-5dce-4a11-888b-c637fda17dd6",
   "metadata": {},
   "source": [
    "3.  How to Predict\n",
    "After training, you can use this model for inference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bc9915-97b5-4739-ac04-52a3e95aa15c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "ner_pipeline = pipeline(\"ner\", model=model, tokenizer=tokenizer, aggregation_strategy=\"simple\")\n",
    "example = \"The model achieved 97% accuracy and 0.88 precision.\"\n",
    "ner_pipeline(example)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ed3a287-dd7a-4c1e-95f8-68b1f8696ed7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\DELL\\AppData\\Roaming\\Python\\Python39\\site-packages\\tf_keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_3440\\550660505.py:94: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected).",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:777\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    776\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_tensor(value):\n\u001b[1;32m--> 777\u001b[0m     tensor \u001b[38;5;241m=\u001b[39m \u001b[43mas_tensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    779\u001b[0m     \u001b[38;5;66;03m# Removing this for now in favor of controlling the shape with `prepend_batch_axis`\u001b[39;00m\n\u001b[0;32m    780\u001b[0m     \u001b[38;5;66;03m# # at-least2d\u001b[39;00m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;66;03m# if tensor.ndim > 2:\u001b[39;00m\n\u001b[0;32m    782\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor.squeeze(0)\u001b[39;00m\n\u001b[0;32m    783\u001b[0m     \u001b[38;5;66;03m# elif tensor.ndim < 2:\u001b[39;00m\n\u001b[0;32m    784\u001b[0m     \u001b[38;5;66;03m#     tensor = tensor[None, :]\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:739\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors.<locals>.as_tensor\u001b[1;34m(value, dtype)\u001b[0m\n\u001b[0;32m    738\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mfrom_numpy(np\u001b[38;5;241m.\u001b[39marray(value))\n\u001b[1;32m--> 739\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mValueError\u001b[0m: expected sequence of length 12 at dim 1 (got 11)",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 105\u001b[0m\n\u001b[0;32m     94\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     95\u001b[0m     model\u001b[38;5;241m=\u001b[39mmodel,\n\u001b[0;32m     96\u001b[0m     args\u001b[38;5;241m=\u001b[39mtraining_args,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    101\u001b[0m     compute_metrics\u001b[38;5;241m=\u001b[39mcompute_metrics\n\u001b[0;32m    102\u001b[0m )\n\u001b[0;32m    104\u001b[0m \u001b[38;5;66;03m# 11. Train\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:2245\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   2243\u001b[0m         hf_hub_utils\u001b[38;5;241m.\u001b[39menable_progress_bars()\n\u001b[0;32m   2244\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2245\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   2246\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2247\u001b[0m \u001b[43m        \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2248\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2249\u001b[0m \u001b[43m        \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   2250\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:2514\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2512\u001b[0m update_step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m   2513\u001b[0m num_batches \u001b[38;5;241m=\u001b[39m args\u001b[38;5;241m.\u001b[39mgradient_accumulation_steps \u001b[38;5;28;01mif\u001b[39;00m update_step \u001b[38;5;241m!=\u001b[39m (total_updates \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m remainder\n\u001b[1;32m-> 2514\u001b[0m batch_samples, num_items_in_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_batch_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_batches\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2515\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, inputs \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(batch_samples):\n\u001b[0;32m   2516\u001b[0m     step \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\trainer.py:5243\u001b[0m, in \u001b[0;36mTrainer.get_batch_samples\u001b[1;34m(self, epoch_iterator, num_batches, device)\u001b[0m\n\u001b[0;32m   5241\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(num_batches):\n\u001b[0;32m   5242\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 5243\u001b[0m         batch_samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mepoch_iterator\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m   5244\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m   5245\u001b[0m         \u001b[38;5;28;01mbreak\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\accelerate\\data_loader.py:566\u001b[0m, in \u001b[0;36mDataLoaderShard.__iter__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    564\u001b[0m \u001b[38;5;66;03m# We iterate one batch ahead to check when we are at the end\u001b[39;00m\n\u001b[0;32m    565\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 566\u001b[0m     current_batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataloader_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    567\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m:\n\u001b[0;32m    568\u001b[0m     \u001b[38;5;28;01myield\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:708\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    705\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    706\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[0;32m    707\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[1;32m--> 708\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    709\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m    710\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m    711\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable\n\u001b[0;32m    712\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    713\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called\n\u001b[0;32m    714\u001b[0m ):\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:764\u001b[0m, in \u001b[0;36m_SingleProcessDataLoaderIter._next_data\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    762\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_next_data\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    763\u001b[0m     index \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_next_index()  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[1;32m--> 764\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_dataset_fetcher\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfetch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mindex\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# may raise StopIteration\u001b[39;00m\n\u001b[0;32m    765\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory:\n\u001b[0;32m    766\u001b[0m         data \u001b[38;5;241m=\u001b[39m _utils\u001b[38;5;241m.\u001b[39mpin_memory\u001b[38;5;241m.\u001b[39mpin_memory(data, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_pin_memory_device)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\torch\\utils\\data\\_utils\\fetch.py:55\u001b[0m, in \u001b[0;36m_MapDatasetFetcher.fetch\u001b[1;34m(self, possibly_batched_index)\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     54\u001b[0m     data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset[possibly_batched_index]\n\u001b[1;32m---> 55\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcollate_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\data_collator.py:46\u001b[0m, in \u001b[0;36mDataCollatorMixin.__call__\u001b[1;34m(self, features, return_tensors)\u001b[0m\n\u001b[0;32m     44\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtf_call(features)\n\u001b[0;32m     45\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m---> 46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtorch_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m return_tensors \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnp\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m     48\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnumpy_call(features)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\data_collator.py:334\u001b[0m, in \u001b[0;36mDataCollatorForTokenClassification.torch_call\u001b[1;34m(self, features)\u001b[0m\n\u001b[0;32m    330\u001b[0m labels \u001b[38;5;241m=\u001b[39m [feature[label_name] \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features] \u001b[38;5;28;01mif\u001b[39;00m label_name \u001b[38;5;129;01min\u001b[39;00m features[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mkeys() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    332\u001b[0m no_labels_features \u001b[38;5;241m=\u001b[39m [{k: v \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m feature\u001b[38;5;241m.\u001b[39mitems() \u001b[38;5;28;01mif\u001b[39;00m k \u001b[38;5;241m!=\u001b[39m label_name} \u001b[38;5;28;01mfor\u001b[39;00m feature \u001b[38;5;129;01min\u001b[39;00m features]\n\u001b[1;32m--> 334\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[43mpad_without_fast_tokenizer_warning\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtokenizer\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43mno_labels_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpadding\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_length\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad_to_multiple_of\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    340\u001b[0m \u001b[43m    \u001b[49m\u001b[43mreturn_tensors\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m    341\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    343\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m labels \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    344\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m batch\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\data\\data_collator.py:67\u001b[0m, in \u001b[0;36mpad_without_fast_tokenizer_warning\u001b[1;34m(tokenizer, *pad_args, **pad_kwargs)\u001b[0m\n\u001b[0;32m     64\u001b[0m tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 67\u001b[0m     padded \u001b[38;5;241m=\u001b[39m tokenizer\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;241m*\u001b[39mpad_args, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mpad_kwargs)\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m     69\u001b[0m     \u001b[38;5;66;03m# Restore the state of the warning.\u001b[39;00m\n\u001b[0;32m     70\u001b[0m     tokenizer\u001b[38;5;241m.\u001b[39mdeprecation_warnings[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAsking-to-pad-a-fast-tokenizer\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m warning_state\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:3407\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.pad\u001b[1;34m(self, encoded_inputs, padding, max_length, pad_to_multiple_of, padding_side, return_attention_mask, return_tensors, verbose)\u001b[0m\n\u001b[0;32m   3404\u001b[0m             batch_outputs[key] \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m   3405\u001b[0m         batch_outputs[key]\u001b[38;5;241m.\u001b[39mappend(value)\n\u001b[1;32m-> 3407\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBatchEncoding\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mreturn_tensors\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:241\u001b[0m, in \u001b[0;36mBatchEncoding.__init__\u001b[1;34m(self, data, encoding, tensor_type, prepend_batch_axis, n_sequences)\u001b[0m\n\u001b[0;32m    237\u001b[0m     n_sequences \u001b[38;5;241m=\u001b[39m encoding[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mn_sequences\n\u001b[0;32m    239\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_n_sequences \u001b[38;5;241m=\u001b[39m n_sequences\n\u001b[1;32m--> 241\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_to_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensor_type\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprepend_batch_axis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\transformers\\tokenization_utils_base.py:793\u001b[0m, in \u001b[0;36mBatchEncoding.convert_to_tensors\u001b[1;34m(self, tensor_type, prepend_batch_axis)\u001b[0m\n\u001b[0;32m    788\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moverflowing_tokens\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    789\u001b[0m             \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    790\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor returning overflowing tokens of different lengths. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    791\u001b[0m                 \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease see if a fast version of this tokenizer is available to have this feature available.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    792\u001b[0m             ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m--> 793\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    794\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mUnable to create tensor, you should probably activate truncation and/or padding with\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    795\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpadding=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtruncation=True\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m to have batched tensors with the same length. Perhaps your\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    796\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m features (`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` in this case) have excessive nesting (inputs type `list` where type `int` is\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    797\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m expected).\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    798\u001b[0m         ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m    800\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[1;31mValueError\u001b[0m: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length. Perhaps your features (`label_ids` in this case) have excessive nesting (inputs type `list` where type `int` is expected)."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from datasets import load_dataset, ClassLabel\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "# 1. Load Dataset from JSON\n",
    "dataset = load_dataset(\"json\", data_files=\"ner_performance_metrics_dataset.json\")[\"train\"]\n",
    "\n",
    "# 2. Extract unique labels and create mappings\n",
    "label_list = sorted({label for example in dataset for label in example[\"ner_tags\"]})\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# 3. Map NER tags to label IDs\n",
    "def encode_labels(example):\n",
    "    example[\"label_ids\"] = [label_to_id[label] for label in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# 4. Load tokenizer and model checkpoint\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(model_checkpoint, num_labels=len(label_list), id2label=id_to_label, label2id=label_to_id)\n",
    "\n",
    "# 5. Tokenization and alignment\n",
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(example[\"label_ids\"][word_idx])\n",
    "        else:\n",
    "            label_ids.append(example[\"label_ids\"][word_idx] if label_all_tokens else -100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# 6. Split dataset\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "# 7. Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# 8. Metric computation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id_to_label[label] for (pred, label) in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[pred] for (pred, label) in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"report\": classification_report(true_labels, true_predictions, output_dict=True),\n",
    "    }\n",
    "\n",
    "# 9. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model_perf_key\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 10. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics\n",
    ")\n",
    "\n",
    "# 11. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9281ac29-a109-44fd-a10c-46e3820bedf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Load the JSON file\n",
    "with open(\"ner_performance_metrics_dataset.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Check for mismatches\n",
    "mismatches = [\n",
    "    i for i, item in enumerate(data)\n",
    "    if len(item[\"tokens\"]) != len(item[\"ner_tags\"])\n",
    "]\n",
    "\n",
    "# Print results\n",
    "if mismatches:\n",
    "    print(f\"Found {len(mismatches)} mismatched entries at indices: {mismatches}\")\n",
    "else:\n",
    "    print(\" All token and ner_tag lists are of equal length.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ead7af2a-4274-4eea-976a-139f48e5fafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification,\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "# 1. Load Dataset from JSON\n",
    "dataset = load_dataset(\"json\", data_files=\"ner_performance_metrics_dataset.json\")[\"train\"]\n",
    "\n",
    "# 2. Extract unique labels and create mappings\n",
    "label_list = sorted({label for example in dataset for label in example[\"ner_tags\"]})\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# 3. Map NER tags to label IDs\n",
    "def encode_labels(example):\n",
    "    example[\"label_ids\"] = [label_to_id[label] for label in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# 4. Load tokenizer and model checkpoint\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    ")\n",
    "\n",
    "# 5. Tokenization and alignment\n",
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        is_split_into_words=True,\n",
    "        max_length=128,\n",
    "    )\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(example[\"label_ids\"][word_idx])\n",
    "        else:\n",
    "            label_ids.append(example[\"label_ids\"][word_idx] if label_all_tokens else -100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    # Manually ensure label_ids match input_ids length\n",
    "    input_len = len(tokenized_inputs[\"input_ids\"])\n",
    "    if len(label_ids) < input_len:\n",
    "        label_ids += [-100] * (input_len - len(label_ids))\n",
    "    elif len(label_ids) > input_len:\n",
    "        label_ids = label_ids[:input_len]\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "def validate_and_fix_labels(example):\n",
    "    labels = example[\"labels\"]\n",
    "    input_len = len(example[\"input_ids\"])\n",
    "\n",
    "    # Flatten nested lists if any\n",
    "    if any(isinstance(lbl, list) for lbl in labels):\n",
    "        flat_labels = []\n",
    "        for lbl in labels:\n",
    "            if isinstance(lbl, list):\n",
    "                flat_labels.extend(lbl)\n",
    "            else:\n",
    "                flat_labels.append(lbl)\n",
    "        labels = flat_labels\n",
    "\n",
    "    # Ensure all labels are ints\n",
    "    labels = [int(lbl) for lbl in labels]\n",
    "\n",
    "    # Pad with -100 if shorter than input_ids\n",
    "    if len(labels) < input_len:\n",
    "        labels = labels + [-100] * (input_len - len(labels))\n",
    "    # Truncate if longer\n",
    "    else:\n",
    "        labels = labels[:input_len]\n",
    "\n",
    "    example[\"labels\"] = labels\n",
    "\n",
    "    # Optional validation print (comment out in production)\n",
    "    print(f\"Input length: {input_len}, Labels length: {len(labels)}\")\n",
    "    print(f\"Labels sample: {labels[:20]}\")\n",
    "\n",
    "    return example\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)\n",
    "# Apply this function to your tokenized dataset\n",
    "tokenized_dataset = tokenized_dataset.map(validate_and_fix_labels)\n",
    "\n",
    "\n",
    "\n",
    "print(tokenized_dataset[0]['labels'])\n",
    "print(type(tokenized_dataset[0]['labels']))\n",
    "\n",
    "\n",
    "# 6. Split dataset\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(train_dataset[0].keys()) \n",
    "\n",
    "\n",
    "# 7. Data collator\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "\n",
    "# 8. Metric computation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_labels = [\n",
    "        [id_to_label[label] for (pred, label) in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[pred] for (pred, label) in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"report\": classification_report(true_labels, true_predictions, output_dict=True),\n",
    "    }\n",
    "\n",
    "# 9. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model_perf_key\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",  # <- fixed typo\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 10. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 11. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54cafc18-b539-44b8-a01e-cfd29e379317",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    print(len(tokenized_dataset[0]['labels']), len(tokenized_dataset[0]['input_ids']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fd9bba7-d73c-408d-a7fe-27d19de62b7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(tokenized_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0afea331-644e-449c-8a13-e978b659dce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all(isinstance(x, int) for x in tokenized_dataset[0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c228285-92fc-4ad4-9992-5c76cf029358",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54376342-8fa4-458a-b5f2-eb1b0bd0a2ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForTokenClassification, # This will handle padding\n",
    ")\n",
    "from datasets import load_dataset\n",
    "from seqeval.metrics import classification_report, f1_score\n",
    "\n",
    "# 1. Load Dataset from JSON\n",
    "dataset = load_dataset(\"json\", data_files=\"ner_performance_metrics_dataset.json\")[\"train\"]\n",
    "\n",
    "# 2. Extract unique labels and create mappings\n",
    "label_list = sorted(list(set(label for example in dataset for label in example[\"ner_tags\"])))\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# 3. Map NER tags to label IDs\n",
    "def encode_labels(example):\n",
    "    example[\"label_ids\"] = [label_to_id[label] for label in example[\"ner_tags\"]]\n",
    "    return example\n",
    "\n",
    "dataset = dataset.map(encode_labels)\n",
    "\n",
    "# 4. Load tokenizer and model checkpoint\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    ")\n",
    "\n",
    "# 5. Tokenization and alignment\n",
    "label_all_tokens = True # Whether to label all sub-tokens or only the first one\n",
    "\n",
    "def tokenize_and_align_labels(examples):\n",
    "    # Do NOT apply padding=\"max_length\" here. Let the DataCollator handle it.\n",
    "    tokenized_inputs = tokenizer(\n",
    "        examples[\"tokens\"],\n",
    "        #truncation=True, # Truncate if longer than max_length\n",
    "        is_split_into_words=True,\n",
    "        padding=True,\n",
    "        #max_length=128, # Set max_length for truncation\n",
    "        # padding=False or padding='do_not_pad' is default behavior without 'padding=True'\n",
    "    )\n",
    "\n",
    "    labels = []\n",
    "    for batch_index, label_ids_example in enumerate(examples[\"label_ids\"]):\n",
    "        word_ids = tokenized_inputs.word_ids(batch_index=batch_index)\n",
    "        previous_word_idx = None\n",
    "        current_labels = []\n",
    "\n",
    "        for word_idx in word_ids:\n",
    "            if word_idx is None:\n",
    "                # Special token (CLS, SEP, PAD) or token not associated with a word\n",
    "                current_labels.append(-100)\n",
    "            elif word_idx != previous_word_idx:\n",
    "                # Start of a new word, label with the corresponding word's label\n",
    "                current_labels.append(label_ids_example[word_idx])\n",
    "            else:\n",
    "                # Continuation of a word (subword token)\n",
    "                # Label according to label_all_tokens\n",
    "                current_labels.append(label_ids_example[word_idx] if label_all_tokens else -100)\n",
    "            previous_word_idx = word_idx\n",
    "        \n",
    "        labels.append(current_labels)\n",
    "    \n",
    "    tokenized_inputs[\"labels\"] = labels\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Apply this function to your dataset\n",
    "# Using batched=True is efficient and allows processing multiple examples at once\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=True)\n",
    "\n",
    "# Verification prints (good for debugging)\n",
    "print(\"Labels for the first example after tokenization and alignment (before DataCollator):\")\n",
    "print(tokenized_dataset[0]['labels'])\n",
    "print(\"Type of labels for the first example:\", type(tokenized_dataset[0]['labels']))\n",
    "print(\"Length of input_ids for the first example:\", len(tokenized_dataset[0]['input_ids']))\n",
    "print(\"Length of labels for the first example:\", len(tokenized_dataset[0]['labels']))\n",
    "\n",
    "\n",
    "# 6. Split dataset\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n",
    "\n",
    "print(\"\\nKeys in the first training example:\")\n",
    "print(train_dataset[0].keys())\n",
    "\n",
    "# 7. Data collator\n",
    "# This is where padding happens now.\n",
    "# By default, DataCollatorForTokenClassification will pad to the longest sequence in the batch.\n",
    "# If you want to force padding to max_length (e.g., 128), you can pass padding=\"max_length\" here,\n",
    "# but usually it's fine to let it pad dynamically per batch.\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer) \n",
    "\n",
    "# 8. Metric computation\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    # Remove ignored index (where label is -100)\n",
    "    true_labels = [\n",
    "        [id_to_label[label] for (pred, label) in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    true_predictions = [\n",
    "        [id_to_label[pred] for (pred, label) in zip(prediction, label_seq) if label != -100]\n",
    "        for prediction, label_seq in zip(predictions, labels)\n",
    "    ]\n",
    "    \n",
    "    # Handle cases where true_labels or true_predictions might be empty\n",
    "    if not any(true_labels) and not any(true_predictions):\n",
    "        return {\"f1\": 1.0, \"report\": \"No true labels or predictions found, returning 1.0 F1\"} # Perfect score if nothing to predict\n",
    "    elif not any(true_labels) or not any(true_predictions):\n",
    "        return {\"f1\": 0.0, \"report\": \"Missing true labels or predictions for F1 computation\"} # Can't compute F1 if one is empty and other isn't\n",
    "    \n",
    "    return {\n",
    "        \"f1\": f1_score(true_labels, true_predictions),\n",
    "        \"report\": classification_report(true_labels, true_predictions, output_dict=True),\n",
    "    }\n",
    "\n",
    "# 9. Training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./ner_model_perf_key\",\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_strategy=\"epoch\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    report_to=\"none\"\n",
    ")\n",
    "\n",
    "# 10. Trainer setup\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=eval_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator, # This is crucial\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 11. Train\n",
    "print(\"\\nStarting training...\")\n",
    "trainer.train()\n",
    "print(\"Training complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "358ccad1-b021-4b7a-8e6a-a98ad88842e6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9eeb8a68d8694541a39af949d100e8f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import DatasetDict\n",
    "\n",
    "label_all_tokens = True\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        is_split_into_words=True,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",  # Ensure consistent length\n",
    "        max_length=128,        # Adjust as needed\n",
    "        return_tensors=None\n",
    "    )\n",
    "\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(example[\"label_ids\"][word_idx])\n",
    "        else:\n",
    "            label_ids.append(example[\"label_ids\"][word_idx] if label_all_tokens else -100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "# Tokenize and align\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, remove_columns=dataset.column_names)\n",
    "\n",
    "# Ensure splits are applied after tokenization\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "eval_dataset = split_dataset[\"test\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "cb2892f7-72d1-4fe4-909d-47358e058027",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " All examples are correctly formatted.\n",
      " All examples are correctly formatted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def validate_tokenized_dataset(dataset, max_len=128):\n",
    "    for idx, example in enumerate(dataset):\n",
    "        labels = example[\"labels\"]\n",
    "        input_ids = example[\"input_ids\"]\n",
    "\n",
    "        # Check 1: labels is a list of ints\n",
    "        if not isinstance(labels, list) or not all(isinstance(x, int) for x in labels):\n",
    "            print(f\"Example {idx}: 'labels' is not a flat list of ints\")\n",
    "            return False\n",
    "\n",
    "        # Check 2: input_ids is same length as labels\n",
    "        if len(labels) != len(input_ids):\n",
    "            print(f\"Example {idx}: length mismatch - labels ({len(labels)}) vs input_ids ({len(input_ids)})\")\n",
    "            return False\n",
    "\n",
    "        # Check 3: length does not exceed max token length\n",
    "        if len(labels) > max_len or len(input_ids) > max_len:\n",
    "            print(f\"Example {idx}: length exceeds max_length ({max_len})\")\n",
    "            return False\n",
    "\n",
    "    print(\" All examples are correctly formatted.\")\n",
    "    return True\n",
    "\n",
    "# Run validation\n",
    "validate_tokenized_dataset(train_dataset)\n",
    "validate_tokenized_dataset(eval_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "64ddb931-146e-4085-bc78-8c3478b10f35",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['input_ids', 'token_type_ids', 'attention_mask', 'labels'],\n",
       "    num_rows: 8\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "8c85c8fa-247d-49bb-b199-9452940ccab5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[101, 1109, 2235, 3890, 1126, 22341, 2794, 1104, 121, 119, 5966, 1113, 1103, 9221, 1891, 1383, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 6, 6, 6, 6, 0, 3, 6, 2, 2, 2, 6, 6, 6, 6, 6, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "\n",
      "\n",
      "[101, 1109, 2235, 3890, 13218, 131, 5429, 119, 126, 117, 9148, 131, 3078, 119, 122, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 6, 6, 6, 0, 6, 2, 2, 2, 6, 0, 6, 2, 2, 2, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "\n",
      "\n",
      "[101, 1109, 10605, 25019, 2941, 23287, 26779, 2235, 18826, 170, 9148, 1104, 5840, 110, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 6, 1, 1, 1, 4, 4, 6, 6, 6, 0, 6, 2, 5, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "\n",
      "\n",
      "[101, 11689, 16073, 1105, 9148, 1127, 1241, 1807, 121, 119, 130, 1111, 19441, 4089, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 0, 0, 6, 0, 6, 6, 6, 2, 2, 2, 6, 1, 4, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "\n",
      "\n",
      "[101, 1556, 170, 155, 9244, 21646, 1658, 1104, 121, 119, 5429, 117, 1103, 2099, 1108, 26192, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 6, 6, 0, 0, 3, 3, 6, 2, 2, 2, 6, 6, 6, 6, 6, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "\n",
      "\n",
      "[101, 3458, 18250, 2443, 2103, 1126, 10893, 2860, 1104, 121, 119, 3078, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 6, 1, 4, 6, 6, 0, 3, 6, 2, 2, 2, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "\n",
      "\n",
      "[101, 161, 13745, 26459, 1204, 12961, 1126, 10893, 1104, 5556, 110, 1107, 1103, 2774, 2233, 9388, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 1, 1, 1, 1, 6, 6, 0, 6, 2, 5, 6, 6, 6, 6, 6, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "\n",
      "\n",
      "[101, 6747, 161, 13745, 26459, 1204, 1125, 1103, 2439, 9148, 1621, 1155, 3584, 1120, 121, 119, 4573, 119, 102, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[-100, 6, 1, 1, 1, 1, 6, 6, 6, 0, 6, 6, 6, 6, 2, 2, 2, 6, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100]\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(8):\n",
    "    print(train_dataset[i]['input_ids'])\n",
    "    print(train_dataset[i]['token_type_ids'])\n",
    "    print(train_dataset[i]['attention_mask'])\n",
    "    print(train_dataset[i]['labels'])\n",
    "    print(\"\\n\\n\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "41f433e8-651e-4449-ae87-c7f1245e613d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "\n",
      "\n",
      "\n",
      "128\n",
      "128\n",
      "128\n",
      "128\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(2):\n",
    "    print(len(eval_dataset[i]['input_ids']))\n",
    "    print(len(eval_dataset[i]['token_type_ids']))\n",
    "    print(len(eval_dataset[i]['attention_mask']))\n",
    "    print(len(eval_dataset[i]['labels']))\n",
    "    print(\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6ec60eb1-dfdb-4482-9270-7cec7969367c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "286e449a0b7d4aa8a9ab9c7449f784f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 128  # or any length suitable for your model\n",
    "\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(\n",
    "        example[\"tokens\"],\n",
    "        truncation=True,\n",
    "        padding='max_length',\n",
    "        max_length=max_length,\n",
    "        is_split_into_words=True\n",
    "    )\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    label_ids = []\n",
    "    previous_word_idx = None\n",
    "\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(example[\"label_ids\"][word_idx])\n",
    "        else:\n",
    "            label_ids.append(example[\"label_ids\"][word_idx] if label_all_tokens else -100)\n",
    "        previous_word_idx = word_idx\n",
    "\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels, batched=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "93dc28e0-0c2a-4d9c-9cb3-ba6986499956",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_collator = DataCollatorForTokenClassification(tokenizer, pad_to_multiple_of=8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "dda11a5e-cd58-437d-8687-fe5cc7a269e0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ba4e1d84a20e46a3bff7e5862bc7d92d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/3 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_3440\\914142802.py:108: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='3' max='3' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [3/3 00:29, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.836830</td>\n",
       "      <td>0.090909</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.333333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.709494</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.645328</td>\n",
       "      <td>0.500000</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.714286</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=3, training_loss=1.8981281916300456, metrics={'train_runtime': 77.421, 'train_samples_per_second': 0.116, 'train_steps_per_second': 0.039, 'total_flos': 45933562080.0, 'train_loss': 1.8981281916300456, 'epoch': 3.0})"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# 1. Example data - Replace with your own dataset.  This is a small example.\n",
    "examples = [\n",
    "    {\n",
    "        \"tokens\": [\"John\", \"Smith\", \"went\", \"to\", \"New\", \"York\", \".\"],\n",
    "        \"ner_tags\": [\"B-PER\", \"I-PER\", \"O\", \"O\", \"B-LOC\", \"I-LOC\", \"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"Apple\", \"Inc.\", \"is\", \"based\", \"in\", \"California\", \".\"],\n",
    "        \"ner_tags\": [\"B-ORG\", \"I-ORG\", \"O\", \"O\", \"O\", \"B-LOC\", \"O\"]\n",
    "    },\n",
    "    {\n",
    "        \"tokens\": [\"The\", \"meeting\", \"is\", \"on\", \"Monday\", \".\"],\n",
    "        \"ner_tags\": [\"O\", \"O\", \"O\", \"O\", \"B-DATE\", \"O\"]\n",
    "    }\n",
    "]\n",
    "\n",
    "# 2. Create Hugging Face dataset\n",
    "dataset = Dataset.from_list(examples)\n",
    "\n",
    "# 3. Define the label list.  Crucially, get this from your data.\n",
    "label_list = sorted(list(set(label for example in examples for label in example[\"ner_tags\"])))\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()} #important for metrics\n",
    "\n",
    "# 4. Tokenizer and model.  Using bert-base-cased here.\n",
    "model_checkpoint = \"bert-base-cased\"  # Change to \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "\n",
    "# 5. Tokenize and align labels\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(label_to_id[example[\"ner_tags\"][word_idx]])\n",
    "        else:\n",
    "            label_ids.append(label_to_id[example[\"ner_tags\"][word_idx]] if True else -100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# 6. Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,  # Add id2label and label2id for correct metrics.\n",
    "    label2id=label_to_id,\n",
    ")\n",
    "\n",
    "# 7. Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner_bert_model\",  # Changed output directory\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 8. Use seqeval metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "\n",
    "# 9. Define compute_metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "\n",
    "# 10. Trainer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 11. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "e1f7be90-f363-4467-8099-b0ff3805d997",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-cased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_3440\\276529196.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='6' max='6' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [6/6 00:22, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.933368</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.441176</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.757539</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.676471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.684891</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.647059</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=6, training_loss=2.0197776158650718, metrics={'train_runtime': 24.6398, 'train_samples_per_second': 0.609, 'train_steps_per_second': 0.244, 'total_flos': 75535873848.0, 'train_loss': 2.0197776158650718, 'epoch': 3.0})"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load data from the JSON file\n",
    "json_file_path = \"ner_dataset.json\"  # Path to your JSON file\n",
    "dataset = load_dataset(\"json\", data_files=json_file_path)[\"train\"]\n",
    "\n",
    "# 2. Define the label list.  Crucially, get this from the dataset.\n",
    "label_list = sorted(list(set(label for example in dataset for label in example[\"ner_tags\"])))\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# 3. Tokenizer and model.  Using bert-base-cased here.\n",
    "model_checkpoint = \"bert-base-cased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 4. Tokenize and align labels\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(label_to_id[example[\"ner_tags\"][word_idx]])\n",
    "        else:\n",
    "            label_ids.append(label_to_id[example[\"ner_tags\"][word_idx]] if True else -100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# 5. Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    ")\n",
    "\n",
    "# 6. Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner_bert_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 7. Use seqeval metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# 8. Define compute_metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# 9. Trainer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 10. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "61f8d25f-c479-4b04-adf4-052135246d52",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a8570905c6fd427092c26d17aa35ee58",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/10 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForTokenClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "C:\\Users\\DELL\\AppData\\Local\\Temp\\ipykernel_3440\\963937540.py:90: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='9' max='9' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [9/9 00:27, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.642740</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>0.491228</td>\n",
       "      <td>0.437500</td>\n",
       "      <td>0.585714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.398331</td>\n",
       "      <td>0.790698</td>\n",
       "      <td>0.596491</td>\n",
       "      <td>0.680000</td>\n",
       "      <td>0.757143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>1.310027</td>\n",
       "      <td>0.868421</td>\n",
       "      <td>0.578947</td>\n",
       "      <td>0.694737</td>\n",
       "      <td>0.757143</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\DELL\\AppData\\Local\\Programs\\Python\\Python39\\lib\\site-packages\\seqeval\\metrics\\v1.py:57: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=9, training_loss=1.66794925265842, metrics={'train_runtime': 42.778, 'train_samples_per_second': 0.701, 'train_steps_per_second': 0.21, 'total_flos': 274578144324.0, 'train_loss': 1.66794925265842, 'epoch': 3.0})"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset, load_dataset\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForTokenClassification,\n",
    "    DataCollatorForTokenClassification,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    ")\n",
    "import evaluate\n",
    "import numpy as np\n",
    "\n",
    "# 1. Load data from the JSON file\n",
    "json_file_path = \"ner_performance_metrics_dataset.json\"  # Path to your JSON file\n",
    "dataset = load_dataset(\"json\", data_files=json_file_path)[\"train\"]\n",
    "\n",
    "# 2. Define the label list.  Crucially, get this from the dataset.\n",
    "label_list = sorted(list(set(label for example in dataset for label in example[\"ner_tags\"])))\n",
    "label_to_id = {label: i for i, label in enumerate(label_list)}\n",
    "id_to_label = {i: label for label, i in label_to_id.items()}\n",
    "\n",
    "# 3. Tokenizer and model.  Using bert-base-cased here.\n",
    "model_checkpoint = \"bert-base-uncased\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "# 4. Tokenize and align labels\n",
    "def tokenize_and_align_labels(example):\n",
    "    tokenized_inputs = tokenizer(example[\"tokens\"], truncation=True, is_split_into_words=True)\n",
    "    word_ids = tokenized_inputs.word_ids()\n",
    "    previous_word_idx = None\n",
    "    label_ids = []\n",
    "    for word_idx in word_ids:\n",
    "        if word_idx is None:\n",
    "            label_ids.append(-100)\n",
    "        elif word_idx != previous_word_idx:\n",
    "            label_ids.append(label_to_id[example[\"ner_tags\"][word_idx]])\n",
    "        else:\n",
    "            label_ids.append(label_to_id[example[\"ner_tags\"][word_idx]] if True else -100)\n",
    "        previous_word_idx = word_idx\n",
    "    tokenized_inputs[\"labels\"] = label_ids\n",
    "    return tokenized_inputs\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize_and_align_labels)\n",
    "\n",
    "# 5. Load model\n",
    "model = AutoModelForTokenClassification.from_pretrained(\n",
    "    model_checkpoint,\n",
    "    num_labels=len(label_list),\n",
    "    id2label=id_to_label,\n",
    "    label2id=label_to_id,\n",
    ")\n",
    "\n",
    "# 6. Training arguments\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"./ner_bert_model\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=4,\n",
    "    per_device_eval_batch_size=4,\n",
    "    num_train_epochs=3,\n",
    "    weight_decay=0.01,\n",
    ")\n",
    "\n",
    "# 7. Use seqeval metric\n",
    "seqeval = evaluate.load(\"seqeval\")\n",
    "\n",
    "# 8. Define compute_metrics\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=2)\n",
    "\n",
    "    true_predictions = [\n",
    "        [label_list[p] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "    true_labels = [\n",
    "        [label_list[l] for (p, l) in zip(prediction, label) if l != -100]\n",
    "        for prediction, label in zip(predictions, labels)\n",
    "    ]\n",
    "\n",
    "    results = seqeval.compute(predictions=true_predictions, references=true_labels)\n",
    "    return {\n",
    "        \"precision\": results[\"overall_precision\"],\n",
    "        \"recall\": results[\"overall_recall\"],\n",
    "        \"f1\": results[\"overall_f1\"],\n",
    "        \"accuracy\": results[\"overall_accuracy\"],\n",
    "    }\n",
    "\n",
    "# 9. Trainer\n",
    "data_collator = DataCollatorForTokenClassification(tokenizer)\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=args,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "# 10. Train\n",
    "trainer.train()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "ab7ad147-8ada-4173-a438-5c02f5cf0173",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "NER Predictions for: 'Accuracy was not so good'\n",
      "accuracy: I-MODEL\n",
      "was: O\n",
      "not: B-METRIC\n",
      "so: B-METRIC\n",
      "good: O\n"
     ]
    }
   ],
   "source": [
    "# 11. Inference function\n",
    "def predict_ner(text, model, tokenizer, id_to_label):\n",
    "    \"\"\"\n",
    "    Predicts NER tags for a given text using the fine-tuned model.\n",
    "\n",
    "    Args:\n",
    "        text (str): The input text to predict NER tags for.\n",
    "        model (AutoModelForTokenClassification): The fine-tuned NER model.\n",
    "        tokenizer (AutoTokenizer): The tokenizer used for the model.\n",
    "        id_to_label (dict): A dictionary mapping label IDs to label names.\n",
    "\n",
    "    Returns:\n",
    "        list: A list of tuples, where each tuple contains a word and its predicted NER tag.\n",
    "    \"\"\"\n",
    "    # Tokenize the input text\n",
    "    inputs = tokenizer(text, return_tensors=\"pt\")  # Tokenize and return PyTorch tensors\n",
    "\n",
    "    # Get the model's predictions\n",
    "    outputs = model(**inputs)\n",
    "    predictions = torch.argmax(outputs.logits, dim=2)  # Get the predicted label IDs\n",
    "\n",
    "    # Convert the predicted IDs to label names\n",
    "    predicted_labels = [id_to_label[t.item()] for t in predictions[0]]  # Use .item()\n",
    "\n",
    "    # Get the input tokens\n",
    "    tokens = tokenizer.convert_ids_to_tokens(inputs[\"input_ids\"][0])  # Use [0] to get the first (and only) sequence\n",
    "\n",
    "    # Filter out special tokens (e.g., [CLS], [SEP]) and align labels\n",
    "    word_tokens = []\n",
    "    aligned_labels = []\n",
    "    for i, token in enumerate(tokens):\n",
    "        if token not in tokenizer.special_tokens_map.values():\n",
    "            word_tokens.append(token)\n",
    "            aligned_labels.append(predicted_labels[i])\n",
    "    \n",
    "    return list(zip(word_tokens, aligned_labels))\n",
    "\n",
    "\n",
    "\n",
    "# 12. Example usage of the inference function\n",
    "import torch\n",
    "new_text = \"Accuracy was not so good\"\n",
    "ner_predictions = predict_ner(new_text, model, tokenizer, id_to_label)\n",
    "print(f\"\\nNER Predictions for: '{new_text}'\")\n",
    "for word, label in ner_predictions:\n",
    "    print(f\"{word}: {label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "2d42150b-7d77-4fba-9fc6-8945b0898dca",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['B-METRIC', 'B-MODEL', 'B-VALUE', 'I-METRIC', 'I-MODEL', 'I-VALUE', 'O'],\n",
       " [('accuracy', 'I-MODEL'),\n",
       "  ('was', 'O'),\n",
       "  ('not', 'B-METRIC'),\n",
       "  ('so', 'B-METRIC'),\n",
       "  ('good', 'O')])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_list, ner_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b116147-b20a-4e33-acd6-7522b7a4237b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
