{"cells":[{"cell_type":"markdown","source":"You're right to focus on Custom Named Entity Recognition (NER) for extracting performance metrics and values, as it's one of the most robust and scalable solutions for this problem, especially when dealing with varied or complex sentence structures.\n\nLet's break down the steps in detail, with a practical example using `spaCy`, a popular and powerful NLP library in Python.\n\n**Goal:** Train a custom NER model to identify \"PERFORMANCE\\_METRIC\" and \"VALUE\" entities in sentences like:\n\n  * \"The **CPU utilization** was **75%**.\"\n  * \"**Latency** improved to **10 ms**.\"\n  * \"**Throughput** reached **1.2 Gbps**.\"\n\n-----\n\n### **Steps for Custom Named Entity Recognition (NER)**\n\n#### **Step 1: Data Annotation (Labeling)**\n\nThis is the most crucial and often the most time-consuming step. You need to manually label examples of your text with the custom entities you want to extract.\n\n**Idea:** For each sentence, identify the exact spans of text that correspond to a `PERFORMANCE_METRIC` and a `VALUE`.\n\n**Format:** spaCy typically expects training data in a list of tuples, where each tuple contains:\n\n1.  The text of the sentence.\n2.  A dictionary containing a list of entities. Each entity is a tuple: `(start_index, end_index, 'ENTITY_TYPE')`.\n\n**Example Data Annotation:**\n\nLet's consider these sentences:\n\n  * \"The CPU utilization was 75%.\"\n  * \"Latency improved to 10 ms.\"\n  * \"Throughput reached 1.2 Gbps on average.\"\n  * \"We observed 99.9% uptime for the service.\"\n  * \"The network bandwidth is 100 Mbps.\"\n\nHere's how you'd annotate them:","metadata":{}},{"cell_type":"code","source":"TRAIN_DATA = [\n    (\"The CPU utilization was 75%.\", {\n        \"entities\": [(4, 19, \"PERFORMANCE_METRIC\"), (24, 28, \"VALUE\")]\n    }),\n    (\"Latency improved to 10 ms.\", {\n        \"entities\": [(0, 7, \"PERFORMANCE_METRIC\"), (20, 25, \"VALUE\")]\n    }),\n    (\"Throughput reached 1.2 Gbps on average.\", {\n        \"entities\": [(0, 10, \"PERFORMANCE_METRIC\"), (19, 27, \"VALUE\")]\n    }),\n    (\"We observed 99.9% uptime for the service.\", {\n        \"entities\": [(12, 17, \"VALUE\"), (18, 23, \"PERFORMANCE_METRIC\")] # Notice order can vary\n    }),\n    (\"The network bandwidth is 100 Mbps.\", {\n        \"entities\": [(4, 20, \"PERFORMANCE_METRIC\"), (24, 32, \"VALUE\")]\n    }),\n    # Add many more examples here... ideally hundreds or thousands for good performance\n    (\"Memory usage peaked at 80%.\", {\n        \"entities\": [(0, 12, \"PERFORMANCE_METRIC\"), (22, 26, \"VALUE\")]\n    }),\n    (\"The disk I/O was 150 MB/s.\", {\n        \"entities\": [(4, 13, \"PERFORMANCE_METRIC\"), (18, 25, \"VALUE\")]\n    }),\n    (\"Response time increased to 300ms.\", {\n        \"entities\": [(0, 12, \"PERFORMANCE_METRIC\"), (24, 30, \"VALUE\")]\n    }),\n    (\"Page load speed is 2.5 seconds.\", {\n        \"entities\": [(0, 15, \"PERFORMANCE_METRIC\"), (19, 30, \"VALUE\")]\n    }),\n    (\"Error rate dropped to 0.1%.\", {\n        \"entities\": [(0, 10, \"PERFORMANCE_METRIC\"), (21, 26, \"VALUE\")]\n    })\n]","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"**Tips for Annotation:**\n\n  * **Consistency:** Be absolutely consistent with your labels. If \"CPU utilization\" is a `PERFORMANCE_METRIC` once, it must always be.\n  * **Exact Spans:** Ensure the start and end indices perfectly cover the entity, without extra spaces or partial words.\n  * **Ambiguity:** If a phrase can sometimes be a metric and sometimes not, you might need more complex rules or more diverse training data.\n  * **Diversity:** Include a wide variety of sentence structures, phrasing, and metrics/values. Don't just use simple \"X is Y\" sentences. Include sentences with conjunctions, different verbs (\"reached\", \"improved\", \"peaked at\", \"dropped to\"), and varied unit formats.\n  * **Quantity:** For a production-ready model, you'd typically need hundreds to thousands of annotated examples. The more varied and representative your data, the better your model will perform.\n\n#### **Step 2: Initialize and Configure the spaCy Model**\n\nYou'll start with a blank spaCy model and add the NER component to it.","metadata":{}},{"cell_type":"code","source":"import spacy\nfrom spacy.training import Example\nimport random\n\n# 1. Create a blank spaCy model\n# You can specify the language, e.g., \"en\" for English\nnlp = spacy.blank(\"en\")\n\n# 2. Add the 'ner' pipeline component\n# This creates an empty NER component if it doesn't exist.\nif \"ner\" not in nlp.pipe_names:\n    ner = nlp.add_pipe(\"ner\")\nelse:\n    ner = nlp.get_pipe(\"ner\")\n\n# 3. Add custom labels to the NER component\nner.add_label(\"PERFORMANCE_METRIC\")\nner.add_label(\"VALUE\")\n\nprint(\"Labels added to NER component:\", ner.labels)","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"#### **Step 3: Prepare the Training Loop**\n\nYou'll iterate multiple times over your training data. Each iteration is called an \"epoch.\" During each epoch, the model adjusts its internal weights based on the annotated examples.","metadata":{}},{"cell_type":"code","source":"# Create a list of Example objects from your TRAIN_DATA\nexamples = []\nfor text, annotations in TRAIN_DATA:\n    examples.append(Example.from_dict(nlp.make_doc(text), annotations))\n\n# Get the names of the pipeline components to disable during training\n# We only want to train the NER component, not others like tagger or parser\npipe_exceptions = [\"ner\", \"trf_wordpiecer\", \"trf_tok2vec\"] # 'trf_' components are for transformer models\nother_pipes = [pipe for pipe in nlp.pipe_names if pipe not in pipe_exceptions]\n\n# Training parameters\nn_iter = 50 # Number of training iterations (epochs)\ndropout = 0.5 # Dropout rate to prevent overfitting","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"#### **Step 4: Train the Model**\n\nThis is where the actual learning happens.","metadata":{}},{"cell_type":"code","source":"# Start the training\nwith nlp.disable_pipes(*other_pipes):  # only train NER\n    optimizer = nlp.begin_training()\n    print(\"Training the model...\")\n    for itn in range(n_iter):\n        random.shuffle(examples) # Shuffle data for better training\n        losses = {}\n        for example in examples:\n            nlp.update(\n                [example],  # batch of examples\n                drop=dropout, # dropout - make it harder to memorise data\n                sgd=optimizer,  # callable to update weights\n                losses=losses,\n            )\n        print(f\"Epoch {itn+1}/{n_iter} - Losses: {losses}\")\n\nprint(\"Training complete!\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"#### **Step 5: Test the Model**\n\nAfter training, you can use the model to predict entities on new, unseen sentences.","metadata":{}},{"cell_type":"code","source":"# Test sentences\ntest_sentences = [\n    \"Our system's uptime is 99.99%.\",\n    \"The new server reduced response time to 50ms.\",\n    \"Data transfer rate peaked at 1.5 Gbps.\",\n    \"We need to monitor CPU usage which is currently 60% and memory consumption at 70%.\",\n    \"What is the network latency?\", # Should ideally not find values if no value\n    \"The average temperature is 25 degrees Celsius.\" # Should not tag as performance metric/value\n]\n\nprint(\"\\n--- Testing the trained model ---\")\nfor text in test_sentences:\n    doc = nlp(text)\n    print(f\"\\nSentence: {doc.text}\")\n    for ent in doc.ents:\n        print(f\"  - Entity: '{ent.text}' | Label: '{ent.label_}'\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"**Expected Output (Example, actual output might vary slightly based on training):**\n\n```\n--- Testing the trained model ---\n\nSentence: Our system's uptime is 99.99%.\n  - Entity: 'uptime' | Label: 'PERFORMANCE_METRIC'\n  - Entity: '99.99%' | Label: 'VALUE'\n\nSentence: The new server reduced response time to 50ms.\n  - Entity: 'response time' | Label: 'PERFORMANCE_METRIC'\n  - Entity: '50ms' | Label: 'VALUE'\n\nSentence: Data transfer rate peaked at 1.5 Gbps.\n  - Entity: 'Data transfer rate' | Label: 'PERFORMANCE_METRIC'\n  - Entity: '1.5 Gbps' | Label: 'VALUE'\n\nSentence: We need to monitor CPU usage which is currently 60% and memory consumption at 70%.\n  - Entity: 'CPU usage' | Label: 'PERFORMANCE_METRIC'\n  - Entity: '60%' | Label: 'VALUE'\n  - Entity: 'memory consumption' | Label: 'PERFORMANCE_METRIC'\n  - Entity: '70%' | Label: 'VALUE'\n\nSentence: What is the network latency?\n  - Entity: 'network latency' | Label: 'PERFORMANCE_METRIC' # Model might pick this up based on training, even without a value\n\nSentence: The average temperature is 25 degrees Celsius.\n # (Ideally, no entities should be found for this, as it's not a performance metric)\n```\n\n#### **Step 6: Save and Load the Model (for production use)**\n\nOnce trained, you'll want to save your model so you don't have to retrain it every time.","metadata":{}},{"cell_type":"code","source":"# Save the model\noutput_dir = \"./custom_ner_model\"\nnlp.to_disk(output_dir)\nprint(f\"\\nModel saved to {output_dir}\")\n\n# Load the model later\nprint(\"\\n--- Loading the saved model ---\")\nloaded_nlp = spacy.load(output_dir)\ndoc = loaded_nlp(\"The network bandwidth is 200 Mbps now.\")\nprint(f\"Sentence: {doc.text}\")\nfor ent in doc.ents:\n    print(f\"  - Entity: '{ent.text}' | Label: '{ent.label_}'\")","outputs":[],"execution_count":null,"metadata":{}},{"cell_type":"markdown","source":"-----\n\n### **Key Considerations and Best Practices for Custom NER:**\n\n1.  **Data Quality and Quantity:**\n\n      * **Garbage In, Garbage Out:** The performance of your NER model is directly proportional to the quality and quantity of your annotated data.\n      * **More Data is Better:** Aim for hundreds, ideally thousands, of diverse examples.\n      * **Balance:** Ensure your training data has a balanced representation of all entity types you want to recognize.\n      * **Real-World Data:** Annotate data that is representative of the text your model will encounter in production.\n\n2.  **Annotation Tools:**\n\n      * For larger datasets, manual annotation becomes tedious. Consider using annotation tools like:\n          * **Prodigy (by spaCy):** A highly efficient annotation tool specifically designed for spaCy.\n          * **Doccano:** An open-source annotation tool for various NLP tasks.\n          * **Label Studio:** Another versatile open-source tool.\n\n3.  **Preprocessing:**\n\n      * While spaCy handles much of it, ensure your input text is clean (e.g., consistent casing if that matters, handling special characters).\n\n4.  **Model Architecture (Advanced):**\n\n      * `spaCy`'s `ner` component is based on a statistical model (often a neural network like a multi-layer perceptron or more recently, transformer-based).\n      * For even higher performance, especially with limited data, you might consider fine-tuning pre-trained language models (like BERT, RoBERTa, etc.) using `Hugging Face Transformers` or `spaCy`'s integration with Transformers. This would involve a different training setup but leverages the massive knowledge embedded in these large models.\n\n5.  **Evaluation:**\n\n      * Always split your annotated data into training, validation, and test sets.\n      * Train on the training set, use the validation set to tune hyperparameters, and *only* evaluate final performance on the unseen test set.\n      * Metrics: Precision, Recall, and F1-score are standard for NER evaluation.\n\n6.  **Iterative Improvement:**\n\n      * NER development is often iterative. Train a first model, analyze its errors, annotate more data based on common error patterns, and retrain.\n\nBy following these detailed steps and best practices, you can successfully build a robust custom NER model to extract performance metrics and values from your text data.","metadata":{}}],"metadata":{"colab":{"from_bard":true},"kernelspec":{"display_name":"Python 3","name":"python3"}},"nbformat":4,"nbformat_minor":0}